{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data():\n",
    "    # Read CSV files into DataFrames\n",
    "    mp_and_pdb = pd.read_csv(\"data/membrane_proteins.csv\")\n",
    "    mp_and_pdb.drop(columns=\"resolution\", inplace=True)\n",
    "    uniprot = pd.read_csv(\"data/membrane_protein_uniprot.csv\")\n",
    "    # uniprot.drop(columns=\"resolution\", inplace=True)\n",
    "    opm = pd.read_csv(\"data/membrane_protein_opm.csv\")\n",
    "    opm.drop(columns=\"resolution\", inplace=True)\n",
    "\n",
    "    # Normalize keys\n",
    "    mp_and_pdb[\"pdb_code\"] = mp_and_pdb[\"pdb_code\"].str.strip().str.upper()\n",
    "    opm[\"pdb_code\"] = opm[\"pdb_code\"].str.strip().str.upper()\n",
    "    mp_and_pdb[\"uniprot_id\"] = mp_and_pdb[\"uniprot_id\"].str.strip().str.upper()\n",
    "    uniprot[\"uniprot_id\"] = uniprot[\"uniprot_id\"].str.strip().str.upper()\n",
    "\n",
    "    # Remove duplicates\n",
    "    mp_and_pdb = mp_and_pdb.drop_duplicates(subset=[\"pdb_code\", \"uniprot_id\"])\n",
    "    opm = opm.drop_duplicates(subset=[\"pdb_code\"])\n",
    "    uniprot = uniprot.drop_duplicates(subset=[\"uniprot_id\"])\n",
    "\n",
    "    # Merge step-by-step\n",
    "    merged_with_opm = pd.merge(mp_and_pdb, opm, on=\"pdb_code\", how=\"inner\", suffixes=(\"_mp\", \"_opm\"))\n",
    "    all_data = pd.merge(merged_with_opm, uniprot, on=\"uniprot_id\", how=\"inner\", suffixes=(\"_opm\", \"_uniprot\"))\n",
    "\n",
    "    return all_data, uniprot, opm, mp_and_pdb\n",
    "all_data, uniprot, opm, mp_and_pdb = load_data()\n",
    "# Inspect the final DataFrame\n",
    "# all_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementation for Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def optimize_knn_imputation(dataframe, n_neighbors_options=[1, 3, 5, 7, 10], cv=5):\n",
    "    numeric_data = dataframe.select_dtypes(include=[np.number])\n",
    "    if numeric_data.empty:\n",
    "        raise ValueError(\"No numeric data available for imputation.\")\n",
    "\n",
    "    # Identify columns with all NaN values\n",
    "    all_nan_columns = numeric_data.columns[numeric_data.isna().all()]\n",
    "    # print(f\"Columns with all NaN values: {list(all_nan_columns)}\")\n",
    "\n",
    "    # Drop columns with all NaN values before imputation\n",
    "    numeric_data_cleaned = numeric_data.drop(columns=all_nan_columns)\n",
    "\n",
    "    # Initialize cross-validation\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    best_score = float('inf')\n",
    "    best_k = None\n",
    "\n",
    "    # Optimize KNNImputer\n",
    "    for k in n_neighbors_options:\n",
    "        imputer = KNNImputer(n_neighbors=k)\n",
    "        try:\n",
    "            # Initialize a list to store scores for this k\n",
    "            fold_scores = []\n",
    "\n",
    "            # Perform K-fold cross-validation\n",
    "            for train_index, test_index in kf.split(numeric_data_cleaned):\n",
    "                train_data, test_data = numeric_data_cleaned.iloc[train_index], numeric_data_cleaned.iloc[test_index]\n",
    "                imputer.fit(train_data)\n",
    "                imputed_data = imputer.transform(test_data)\n",
    "\n",
    "                # Calculate mean squared error for this fold\n",
    "                mse = np.mean((imputed_data - test_data) ** 2)\n",
    "                fold_scores.append(mse)\n",
    "\n",
    "            # Compute the mean MSE for this k\n",
    "            mean_score = np.mean(fold_scores)\n",
    "            # print(f\"K={k}, Mean Squared Error: {mean_score}\")\n",
    "\n",
    "            if mean_score < best_score:\n",
    "                best_score = mean_score\n",
    "                best_k = k\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during cross-validation for k={k}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Use the best K for imputation\n",
    "    best_k = best_k if best_k else n_neighbors_options[0]\n",
    "    imputer = KNNImputer(n_neighbors=best_k)\n",
    "    imputed_data = imputer.fit_transform(numeric_data_cleaned)\n",
    "\n",
    "    # Create DataFrame from imputed data\n",
    "    imputed_df = pd.DataFrame(imputed_data, index=numeric_data.index, columns=numeric_data_cleaned.columns)\n",
    "\n",
    "    # Add back columns with all NaN values\n",
    "    for col in all_nan_columns:\n",
    "        imputed_df[col] = np.nan\n",
    "\n",
    "    # Ensuring the column order matches the original DataFrame\n",
    "    imputed_df = imputed_df[numeric_data.columns]\n",
    "\n",
    "    return imputed_df, {\"n_neighbors\": best_k, \"mean_score\": best_score}\n",
    "\n",
    "def impute_and_merge(categorical_data, numeric_data, categorical_n_neighbors=[1, 3, 5, 7], numeric_n_neighbors=[1, 3, 5, 7]):\n",
    "    # Impute numerical data\n",
    "    numeric_imputed, numeric_model = optimize_knn_imputation(numeric_data, numeric_n_neighbors)\n",
    "\n",
    "    # Impute categorical data (treat one-hot encoded as numerical)\n",
    "    categorical_imputed, categorical_model = optimize_knn_imputation(categorical_data, categorical_n_neighbors)\n",
    "\n",
    "    # Merge imputed datasets\n",
    "    merged_data = pd.concat([categorical_imputed, numeric_imputed], axis=1)\n",
    "\n",
    "    return merged_data, {\"categorical_model\": categorical_model, \"numeric_model\": numeric_model}\n",
    "\n",
    "\n",
    "def onehot_encoder(data, columns=None, drop=None, exempt_columns=None):\n",
    "    if columns is None:\n",
    "        # Automatically detect categorical columns if not provided\n",
    "        columns = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # Exclude exempt columns from encoding\n",
    "    exempt_columns = exempt_columns or []\n",
    "    columns_to_encode = [col for col in columns if col not in exempt_columns]\n",
    "\n",
    "    # Extract exempted columns\n",
    "    exempted_data = data[exempt_columns] if exempt_columns else pd.DataFrame(index=data.index)\n",
    "\n",
    "    # One-hot encode remaining columns\n",
    "    if columns_to_encode:\n",
    "        encoder = OneHotEncoder(drop=drop, sparse_output=False)\n",
    "        encoded_data = encoder.fit_transform(data[columns_to_encode])\n",
    "        encoded_columns = encoder.get_feature_names_out(columns_to_encode)\n",
    "        encoded_df = pd.DataFrame(encoded_data, columns=encoded_columns, index=data.index)\n",
    "    else:\n",
    "        encoded_df = pd.DataFrame(index=data.index)\n",
    "\n",
    "    # Combine exempted columns and encoded data\n",
    "    final_df = pd.concat([encoded_df, exempted_data], axis=1)\n",
    "    return final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Jobs.transformData import report_and_clean_missing_values\n",
    "from utils.package import drop_columns, drop_id_columns, impute_knn_best_hyperparameter, separate_numerical_categorical\n",
    "\n",
    "\n",
    "# 1. separate data into numerical and categorical\n",
    "# 2. Remove columns with emptiness based on percentage\n",
    "# 3. One hot encode categorical data\n",
    "# 4. Impute missing values\n",
    "# Step 1: Clean and preprocess the data\n",
    "clean_df = report_and_clean_missing_values(all_data, threshold=30)\n",
    "# clean_df = clean_df[clean_df[\"rcsentinfo_experimental_method\"] == \"EM\"]\n",
    "\n",
    "# Step 2: Separate numerical and categorical data\n",
    "numerical_cols, categorical_cols = separate_numerical_categorical(clean_df)\n",
    "\n",
    "categorical_data = drop_id_columns(clean_df[categorical_cols])\n",
    "categorical_data = drop_columns(\n",
    "    categorical_data,\n",
    "    [\n",
    "        \"is_master_protein\", \"pdb_code_opm\", \"name_mp\", \"description\",\n",
    "        \"related_pdb_entries\", \"rcsentinfo_na_polymer_entity_types\",\n",
    "        \"rcsb_primary_citation_country\", \"citation_country\", \"pdbid\",\n",
    "        \"name_opm\", \"group\", \"subgroup\", \"molecular_function\",\n",
    "        \"cellular_component\", \"biological_process\", \"species_description\",\n",
    "        \"species_name\", \"family_superfamily_classtype_name\",\n",
    "        \"family_superfamily_name\", \"family_name\",\n",
    "        \"rcsentinfo_selected_polymer_entity_types\", \"expressed_in_species\",\n",
    "        \"taxonomic_domain\", \"species\", \"rcsentinfo_experimental_method\",\n",
    "        \"species_name_cache\", \"membrane_short_name\", \"membrane_name_cache\",\n",
    "        \"species_name_cache\", \"famsupclasstype_type_name\",\n",
    "        \"family_name_cache\", \"exptl_method\", \"rcsentinfo_polymer_composition\",\n",
    "        \"exptl_crystal_grow_method\", \"exptl_crystal_grow_method1\", \"membrane_name\",\n",
    "        \"info_created\", \"expcrygrow_pdbx_details\", \"info_sequence_update\", \"organism_scientific_name\",\n",
    "        \"organism_common_name\", \"organism_lineage\", \"secondary_accession\", \"protein_recommended_name\",\n",
    "        \"associated_genes\", \"sequence_sequence\", \"protein_alternative_name\",\n",
    "        \"rcsentinfo_diffrn_resolution_high_provenance_source\", \"symspagroup_name_hm\",\n",
    "        \"difradpdbx_scattering_type\", \"difradpdbx_scattering_type\", \"info_type\",\n",
    "        \"info_modified\", \"membrane_topology_out\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "numeric_data = drop_id_columns(clean_df[numerical_cols])\n",
    "\n",
    "# Step 3: One-hot encode categorical data\n",
    "# Specify exempt columns\n",
    "exempt_cols = [\"pdb_code_uniprot\"]\n",
    "\n",
    "# One-hot encode with exemptions\n",
    "onehot_data = onehot_encoder(categorical_data, exempt_columns=exempt_cols)\n",
    "\n",
    "# Step 4: Impute and merge\n",
    "final_data, models_info = impute_and_merge(onehot_data, numeric_data)\n",
    "\n",
    "# Display results\n",
    "# print(\"Final Imputed DataFrame:\\n\", final_data.head())\n",
    "# print(\"Imputation Models Info:\", models_info)\n",
    "\n",
    "\n",
    "# all_processed_data = pd.concat([onehot_data, fixed_missing_data], axis=1)\n",
    "final_data.drop([\"bibliography_year\"], axis=1, inplace=True)\n",
    "all_processed_data = final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def calculate_feature_importance(model, feature_names, plot=True, top_n=100):\n",
    "    # Check if the model has feature_importances_\n",
    "    if not hasattr(model, \"feature_importances_\"):\n",
    "        raise ValueError(\"The model does not have a `feature_importances_` attribute.\")\n",
    "\n",
    "    # Extract feature importance\n",
    "    importance = model.feature_importances_\n",
    "    # Create a DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    if plot:\n",
    "        # Plot the top_n features\n",
    "        importance_df.head(top_n).sort_values(by='Importance').plot(\n",
    "            kind='barh', x='Feature', y='Importance', legend=False, figsize=(100, 150)\n",
    "        )\n",
    "        plt.title(\"Top Feature Importances\", fontsize=60)\n",
    "        plt.xlabel(\"Importance\", fontsize=100)\n",
    "        plt.ylabel(\"Feature\", fontsize=100)\n",
    "        plt.xticks(fontsize=100)\n",
    "        plt.yticks(fontsize=100)\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "# Could test with these features as well , \"subgroup\", \"family_name\", \"taxonomic_domain\"\n",
    "all_processed_data_with_labels = pd.concat([all_processed_data, clean_df[[\"group\"]]], axis=1)\n",
    "\n",
    "feature_names = all_processed_data.columns\n",
    "# Train a model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(all_processed_data, clean_df[[\"group\"]])\n",
    "\n",
    "# Calculate and visualize feature importance\n",
    "feature_importance_df = calculate_feature_importance(model, feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Selections from the chart above to make more informed decision for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processed_data.drop([\n",
    "    \"rcsentinfo_assembly_count\",\n",
    "    \"rcsentinfo_branched_entity_count\", \"rcsentinfo_cis_peptide_count\",\n",
    "    \"rcsentinfo_deposited_atom_count\",\"rcsentinfo_deposited_deuterated_water_count\",\n",
    "    \"rcsentinfo_deposited_hydrogen_atom_count\",\"rcsentinfo_deposited_model_count\",\n",
    "    \"rcsentinfo_deposited_modeled_polymer_monomer_count\",\"rcsentinfo_deposited_nonpolymer_entity_instance_count\",\n",
    "    \"rcsentinfo_deposited_polymer_entity_instance_count\",\"rcsentinfo_deposited_polymer_monomer_count\",\n",
    "    \"rcsentinfo_deposited_solvent_atom_count\",\"rcsentinfo_deposited_unmodeled_polymer_monomer_count\",\n",
    "    \"rcsentinfo_disulfide_bond_count\",\"rcsentinfo_entity_count\",\"rcsentinfo_experimental_method_count\",\n",
    "    \"rcsentinfo_inter_mol_covalent_bond_count\",\"rcsentinfo_inter_mol_metalic_bond_count\",\n",
    "    \"rcsentinfo_molecular_weight\",\"rcsentinfo_nonpolymer_entity_count\",\"rcsentinfo_nonpolymer_molecular_weight_maximum\",\n",
    "    \"rcsentinfo_nonpolymer_molecular_weight_minimum\",\"rcsentinfo_polymer_entity_count\",\n",
    "    \"rcsentinfo_polymer_entity_count_dna\",\"rcsentinfo_polymer_entity_count_rna\",\n",
    "    \"rcsentinfo_polymer_entity_count_nucleic_acid\",\"rcsentinfo_polymer_entity_count_protein\",\n",
    "    \"rcsentinfo_polymer_entity_taxonomy_count\",\"rcsentinfo_polymer_molecular_weight_maximum\",\n",
    "    \"rcsentinfo_polymer_molecular_weight_minimum\",\"rcsentinfo_polymer_monomer_count_maximum\",\n",
    "    \"rcsentinfo_polymer_monomer_count_minimum\",\n",
    "    \"rcsentinfo_solvent_entity_count\",\n",
    "    # \"processed_resolution\",\n",
    "    # \"thickness\",\n",
    "    \"thicknesserror\",\n",
    "    # \"subunit_segments\",\"tilt\",\n",
    "    \"tilterror\",\n",
    "    # \"gibbs\",\n",
    "    \"annotation_score\",\n",
    "    # \"sequence_length\",\"sequence_mass\"\n",
    "], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Confirm our selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "from utils.package import evaluate_dimensionality_reduction\n",
    "import altair as alt\n",
    "\n",
    "methods_params = {\n",
    "    'PCA': {'n_components': 2},\n",
    "    't-SNE': {'perplexity': 50},\n",
    "    'UMAP': {'n_neighbors': 15}\n",
    "}\n",
    "\n",
    "reduced_data, plot_data = evaluate_dimensionality_reduction(all_processed_data, methods_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_plot_data = pd.concat(plot_data)\n",
    "\n",
    "# append categorical data\n",
    "other_label = clean_df[[\"group\", \"subgroup\", \"family_name\", \"taxonomic_domain\"]]\n",
    "categorical_data = categorical_data.reset_index(drop=True)\n",
    "combined_plot_data = pd.concat(plot_data).reset_index(drop=True)\n",
    "data_combined_PCA = pd.concat([categorical_data, combined_plot_data[combined_plot_data[\"Method\"] == \"PCA\"], other_label], axis=1)\n",
    "\n",
    "# Plot using Altair\n",
    "alt.Chart(data_combined_PCA).mark_circle().encode(\n",
    "    x='Component 1',\n",
    "    y='Component 2',\n",
    "    color='group',\n",
    "    tooltip=['Method', 'Parameter']\n",
    ").transform_filter(\n",
    "    (alt.datum.Method == \"PCA\") & (alt.datum.group != None)\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=400\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined_t_SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.disable_max_rows()\n",
    "t_sne = combined_plot_data[combined_plot_data[\"Method\"] == \"t-SNE\"].reset_index()\n",
    "data_combined_t_SNE = pd.concat([categorical_data, t_sne, other_label], axis=1)\n",
    "\n",
    "alt.Chart(data_combined_t_SNE).mark_circle().encode(\n",
    "    x='Component 1',\n",
    "    y='Component 2',\n",
    "    color='group',\n",
    "    tooltip=['Method', 'Parameter', 'pdb_code_uniprot']\n",
    ").transform_filter(\n",
    "    (alt.datum.Method == \"t-SNE\") & (alt.datum.group != None)\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=400\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.disable_max_rows()\n",
    "umap_data = combined_plot_data[combined_plot_data[\"Method\"] == \"UMAP\"].reset_index()\n",
    "data_combined_UMAP = pd.concat([categorical_data, umap_data, other_label], axis=1)\n",
    "\n",
    "alt.Chart(data_combined_UMAP).mark_circle().encode(\n",
    "    x='Component 1',\n",
    "    y='Component 2',\n",
    "    color='group',\n",
    "    tooltip=['Method', 'Parameter']\n",
    ").transform_filter(\n",
    "    (alt.datum.Method == \"UMAP\")\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=400\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    cross_validate, \n",
    "    StratifiedKFold,\n",
    "    KFold\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, \n",
    "    f1_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    accuracy_score\n",
    ")\n",
    "from app import app\n",
    "import altair as alt\n",
    "from database.db import db\n",
    "from src.Dashboard.services import get_tables_as_dataframe, get_table_as_dataframe\n",
    "from src.Jobs.Utils import (\n",
    "    ClassifierComparison, onehot_encoder, ClassifierComparisonSemiSupervised,\n",
    "    select_features_using_decision_tree, separate_numerical_categorical,\n",
    "    evaluate_dimensionality_reduction\n",
    ")\n",
    "from src.Jobs.transformData import report_and_clean_missing_values\n",
    "\n",
    "class MLJob:\n",
    "    def __init__(self):\n",
    "        self.num_runs = 1  # Define the number of runs for averaging metrics\n",
    "        self.random_state = 42  # For reproducibility\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        # Load data\n",
    "        self.load_data()\n",
    "        self.models = {}  # Initialize self.models as an empty dictionary\n",
    "        # Data containers\n",
    "        self.data = pd.DataFrame()\n",
    "        self.numerical_data = pd.DataFrame()\n",
    "        self.categorical_data = pd.DataFrame()\n",
    "        self.complete_numerical_data = pd.DataFrame()\n",
    "        self.data_combined_PCA = pd.DataFrame()\n",
    "        self.data_combined_tsne = pd.DataFrame()\n",
    "        self.data_combined_UMAP = pd.DataFrame()\n",
    "        self.semi_supervised_metrics = pd.DataFrame()\n",
    "        self.supervised_metrics = pd.DataFrame()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.over_sampling_data_selected_feature_data = pd.DataFrame()\n",
    "        \n",
    "    def load_data(self):\n",
    "        self.result_df, self.result_df_uniprot, self.result_df_opm, self.result_df_db = load_data()\n",
    "        # print(result_df, result_df_uniprot, result_df_opm, result_df_db)\n",
    "        self.all_data = pd.merge(self.result_df_db, self.result_df_opm, on=\"pdb_code\", how=\"inner\", suffixes=(\"_mp\", \"_opm\"))\n",
    "        # Reset index before filtering\n",
    "        self.all_data = self.all_data.reset_index(drop=True)\n",
    "        # Reserve some data for test specifically for discrepancies\n",
    "        exclude_pdb_codes = [\n",
    "            \"1PFO\", \"1B12\", \"1GOS\", \"1MT5\", \"1KN9\", \"1OJA\", \"1O5W\", \"1UUM\", \"1T7D\", \"2BXR\",\n",
    "            \"1YGM\", \"2GMH\", \"2OLV\", \"2OQO\", \"2QCU\", \"2PRM\", \"2Z5X\", \"2VQG\", \"3HYW\", \"3I65\",\n",
    "            \"3VMA\", \"3NSJ\", \"3ML3\", \"3PRW\", \"3P1L\", \"3Q7M\", \"2YH3\", \"3LIM\", \"3VMT\", \"3Q54\",\n",
    "            \"2YMK\", \"2LOU\", \"4LXJ\", \"4HSC\", \"4CDB\", \"4P6J\", \"4TSY\", \"5B49\", \"5IMW\", \"5IMY\",\n",
    "            \"5JYN\", \"5LY6\", \"6BFG\", \"6MLU\", \"6DLW\", \"6H03\", \"6NYF\", \"6MTI\", \"7OFM\", \"7LQ6\",\n",
    "            \"7RSL\", \"8A1D\", \"7QAM\"\n",
    "        ]\n",
    "        \n",
    "        self.all_data = self.all_data[~self.all_data['pdb_code'].isin(exclude_pdb_codes)]\n",
    "        \n",
    "    \n",
    "    def fix_missing_data(self):\n",
    "        self.data = report_and_clean_missing_values(self.all_data, threshold=30)\n",
    "        columns_to_drop = [col for col in self.data.columns if '_citation_' in col or '_count_' in col or col.startswith('count_') or col.endswith('_count') or col.startswith('revision_') or col.endswith('_revision') or col.startswith('id_') or col.endswith('_id') or col == \"id\"]\n",
    "        self.data.drop(columns_to_drop + [\n",
    "            'pdbid', 'name_mp', 'name_opm', 'tilterror', 'description', 'family_name', \n",
    "            'species_name', 'exptl_method', 'thicknesserror', 'citation_country', \n",
    "            'family_name_cache', \"bibliography_year\", 'is_master_protein', \n",
    "            'species_name_cache', 'membrane_name_cache', 'species_description', \n",
    "            'membrane_short_name', \"processed_resolution\", 'family_superfamily_name', \n",
    "            'famsupclasstype_type_name', 'exptl_crystal_grow_method', \n",
    "            'exptl_crystal_grow_method1', 'family_superfamily_classtype_name', \n",
    "            'rcsentinfo_nonpolymer_molecular_weight_maximum', \n",
    "            'rcsentinfo_nonpolymer_molecular_weight_minimum', \n",
    "            \"rcsentinfo_polymer_molecular_weight_minimum\", \"rcsentinfo_molecular_weight\", \n",
    "            \"rcsentinfo_polymer_molecular_weight_maximum\", 'gibbs'\n",
    "        ], axis=1, inplace=True)\n",
    "        return self\n",
    "\n",
    "    def variable_separation(self):\n",
    "        data = self.data.dropna()\n",
    "        numerical_cols, categorical_cols = separate_numerical_categorical(data)\n",
    "        self.numerical_data = data[numerical_cols]\n",
    "        self.categorical_data = data[categorical_cols]\n",
    "        return self\n",
    "    \n",
    "    def feature_selection(self):\n",
    "        self.categorical_data.reset_index(drop=True, inplace=True)\n",
    "        encode_data = self.categorical_data[[\"membrane_topology_in\", \"membrane_topology_out\"]]\n",
    "        # encoded_data = onehot_encoder(encode_data)\n",
    "        # encoded_data.reset_index(drop=True, inplace=True)\n",
    "        encoded_data = pd.DataFrame([])\n",
    "        encoded_data['membrane_topology_in'] = self.label_encoder.fit_transform(self.categorical_data['membrane_topology_in'])\n",
    "        encoded_data['membrane_topology_out'] = self.label_encoder.fit_transform(self.categorical_data['membrane_topology_out'])\n",
    "        \n",
    "        \n",
    "        self.numerical_data.reset_index(drop=True, inplace=True)\n",
    "        self.complete_numerical_data = pd.concat([self.numerical_data, encoded_data], axis=1).reset_index(drop=True)\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "        y = self.categorical_data[\"group\"]\n",
    "        y_encoded = label_encoder.fit_transform(y)\n",
    "        y_data_frame = pd.DataFrame(y_encoded, columns=['group']).reset_index(drop=True)\n",
    "        \n",
    "        combined_data = pd.concat([self.complete_numerical_data, y_data_frame], axis=1).dropna()\n",
    "        self.complete_numerical_data, top_features = select_features_using_decision_tree(combined_data, target_column='group', num_features=30)\n",
    "        print(top_features)\n",
    "        \n",
    "        self.over_sampling_data_selected_feature_data = pd.concat([self.complete_numerical_data, y], axis=1)\n",
    "        \n",
    "        raw_data = pd.concat([\n",
    "            self.numerical_data, \n",
    "            self.categorical_data[[\n",
    "                    \"pdb_code\", \"membrane_topology_in\", \n",
    "                    \"membrane_topology_out\",\n",
    "                    \"group\"\n",
    "                ]\n",
    "            ]], axis=1)\n",
    "        raw_data.reset_index(drop=True, inplace=True)\n",
    "        raw_data.to_csv(\"./models/semi-supervised/without_reduction_data.csv\", index=False)\n",
    "        return self\n",
    "        \n",
    "    def dimensionality_reduction(self):\n",
    "        methods_params = {\n",
    "            'PCA': {'n_components': 2},\n",
    "            't-SNE': {'n_components': 2, 'perplexity': 30},\n",
    "            'UMAP': {'n_components': 2, 'n_neighbors': 15}\n",
    "        }\n",
    "        self.complete_numerical_data = self.over_sampling_data_selected_feature_data.iloc[:, :-1]\n",
    "        categorical_data = self.over_sampling_data_selected_feature_data[\"group\"]\n",
    "        \n",
    "        reduced_data, plot_data = evaluate_dimensionality_reduction(self.complete_numerical_data, methods_params)\n",
    "        combined_plot_data = pd.concat(plot_data)\n",
    "        \n",
    "        self.data_combined_PCA = pd.concat([combined_plot_data[combined_plot_data[\"Method\"] == \"PCA\"].reset_index(drop=True), categorical_data], axis=1)\n",
    "        self.data_combined_PCA.to_csv(\"./models/semi-supervised/PCA_data.csv\", index=False)\n",
    "        \n",
    "        self.data_combined_tsne = pd.concat([combined_plot_data[combined_plot_data[\"Method\"] == \"t-SNE\"].reset_index(drop=True), categorical_data], axis=1)\n",
    "        self.data_combined_tsne.to_csv(\"./models/semi-supervised/TSNE_data.csv\", index=False)\n",
    "        \n",
    "        self.data_combined_UMAP = pd.concat([combined_plot_data[combined_plot_data[\"Method\"] == \"UMAP\"].reset_index(drop=True), categorical_data], axis=1)\n",
    "        self.data_combined_UMAP.to_csv(\"./models/semi-supervised/UMAP_data.csv\", index=False)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def plot_charts(self):\n",
    "        chart_list = {\n",
    "            \"pca\": self.data_combined_PCA,\n",
    "            \"tsne\": self.data_combined_tsne,\n",
    "            \"umap\": self.data_combined_UMAP\n",
    "        }\n",
    "        \n",
    "        for key, obj in chart_list.items():\n",
    "            chart = alt.Chart(obj).mark_circle().encode(\n",
    "                x='Component 1',\n",
    "                y='Component 2',\n",
    "                color='group',\n",
    "                tooltip=[\"group\"]\n",
    "            ).properties(width=800, height=500)\n",
    "            chart.save('models/' + key + '.png', scale_factor=2.0)\n",
    "        return self\n",
    "\n",
    "    def run_classificationXXX(self, X, y, model_class, filename_prefix, X_unlabeled=None):\n",
    "        \"\"\"Run classification and save results.\"\"\"\n",
    "        metrics_list = []\n",
    "\n",
    "        for run in range(self.num_runs):\n",
    "            if X_unlabeled is not None:  # Semi-Supervised Case\n",
    "                model = model_class(X, y, X_unlabeled, test_size=0.2, random_state=self.random_state + run)\n",
    "            else:  # Supervised Case\n",
    "                model = model_class(X, y, test_size=0.2, random_state=self.random_state + run)\n",
    "\n",
    "            # Train and evaluate the models\n",
    "            model.train_and_evaluate()\n",
    "\n",
    "            # Collect metrics for aggregation\n",
    "            metrics_list.append(model.results_df)\n",
    "\n",
    "            # Save model and plot performance\n",
    "            model.save_models(save_filename=f\"{filename_prefix}_{run}\")\n",
    "            model.plot_performance_comparison(save_filename=f\"{filename_prefix}_{run}\")\n",
    "\n",
    "        # Concatenate all metric results\n",
    "        concatenated_metrics = pd.concat(metrics_list)\n",
    "\n",
    "        # Select only numeric columns for aggregation\n",
    "        numeric_columns = concatenated_metrics.select_dtypes(include=['number'])\n",
    "\n",
    "        # Aggregate only numeric columns\n",
    "        aggregated_metrics = numeric_columns.groupby(level=0).agg(['mean', 'std'])\n",
    "\n",
    "        # Combine non-numeric data with aggregated numeric data (if needed)\n",
    "        non_numeric_columns = concatenated_metrics.select_dtypes(exclude=['number']).drop_duplicates()\n",
    "        if not non_numeric_columns.empty:\n",
    "            aggregated_metrics = pd.concat([aggregated_metrics, non_numeric_columns], axis=1)\n",
    "\n",
    "        # Save aggregated metrics\n",
    "        aggregated_metrics.to_csv(f\"./models/{filename_prefix}_metrics_mean.csv\")\n",
    "        print(f\"Metrics saved to ./models/{filename_prefix}_metrics_mean.csv\")      \n",
    "    \n",
    "    \n",
    "    def plot_metrics_altair(self, metrics_data):\n",
    "        # Convert metrics_data to a DataFrame\n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "        \n",
    "        # Melt the DataFrame to long format for Altair\n",
    "        plot_data = metrics_df.melt(id_vars='Classifier', value_vars=['Mean Accuracy', 'Mean F1-Score', 'Mean Precision', 'Mean Recall'],\n",
    "                                    var_name='Metric', value_name='Score')\n",
    "        \n",
    "        # Create a grouped bar chart with Altair\n",
    "        chart = alt.Chart(plot_data).mark_bar().encode(\n",
    "            x=alt.X('Classifier:N', title='Classifier'),\n",
    "            y=alt.Y('Score:Q', title='Score'),\n",
    "            color=alt.Color('Metric:N', title='Metric'),\n",
    "            column=alt.Column('Metric:N', title='Metric')\n",
    "        ).properties(\n",
    "            title='Mean Performance Metrics for Each Classifier',\n",
    "            width=200,\n",
    "            height=300\n",
    "        ).configure_axis(\n",
    "            labelAngle=-45\n",
    "        ).configure_view(\n",
    "            stroke='transparent'\n",
    "        )\n",
    "        \n",
    "        # Save the plot as a file\n",
    "        chart.save('./models/metrics_comparison_altair.png')\n",
    "        \n",
    "    def run_classification(self, X, y, model_class, filename_prefix, X_unlabeled=None):\n",
    "        \"\"\"Run classification and save results.\"\"\"\n",
    "        metrics_list = []\n",
    "        best_model = None\n",
    "        best_score = -float('inf')  # Initialize with a very low value\n",
    "        # Create an empty list to collect metrics for saving to CSV\n",
    "        metrics_data = []\n",
    "        \n",
    "        for run in range(self.num_runs):\n",
    "            if X_unlabeled is not None:  # Semi-Supervised Case\n",
    "                model = model_class(X, y, X_unlabeled, test_size=0.2, random_state=self.random_state + run)\n",
    "            else:  # Supervised Case\n",
    "                model = model_class(X, y, test_size=0.2, random_state=self.random_state + run)\n",
    "\n",
    "            # Train and evaluate the models\n",
    "            model.train_and_evaluate()\n",
    "            # Define stratified cross-validation\n",
    "            # skf = StratifiedKFold(n_splits=5)\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=self.random_state + run)\n",
    "            # Perform cross-validation for each classifier using multiple metrics\n",
    "            for clf_name, clf in model.models.items():\n",
    "                scoring = {\n",
    "                    'accuracy': make_scorer(accuracy_score),\n",
    "                    'f1_weighted': make_scorer(f1_score, average='weighted', zero_division=0),\n",
    "                    'precision_weighted': make_scorer(precision_score, average='weighted', zero_division=0),\n",
    "                    'recall_weighted': make_scorer(recall_score, average='weighted', zero_division=0)\n",
    "                }\n",
    "                \n",
    "                cv_results = cross_validate(clf, X, y, cv=5, scoring=scoring, return_train_score=False)\n",
    "                \n",
    "                # Calculate mean scores for each metric\n",
    "                mean_accuracy = cv_results['test_accuracy'].mean()\n",
    "                mean_f1 = cv_results['test_f1_weighted'].mean()\n",
    "                mean_precision = cv_results['test_precision_weighted'].mean()\n",
    "                mean_recall = cv_results['test_recall_weighted'].mean()\n",
    "\n",
    "                # Collect metrics data for saving to CSV\n",
    "                metrics_data.append({\n",
    "                    'Run': run,\n",
    "                    'Classifier': clf_name,\n",
    "                    'Accuracy Scores': cv_results['test_accuracy'],\n",
    "                    'Mean Accuracy': mean_accuracy,\n",
    "                    'F1-Score': cv_results['test_f1_weighted'],\n",
    "                    'Mean F1-Score': mean_f1,\n",
    "                    'Precision': cv_results['test_precision_weighted'],\n",
    "                    'Mean Precision': mean_precision,\n",
    "                    'Recall': cv_results['test_recall_weighted'],\n",
    "                    'Mean Recall': mean_recall\n",
    "                })\n",
    "\n",
    "                # Evaluate and update the best model based on chosen metric\n",
    "                if mean_f1 > best_score:\n",
    "                    best_score = mean_f1\n",
    "                    best_model = clf\n",
    "                    best_clf_name = clf_name\n",
    "\n",
    "                # Collect metrics for aggregation\n",
    "                metrics_list.append(model.results_df)\n",
    "\n",
    "            # Save models and plot performance only after all runs\n",
    "            model.save_models(save_filename=f\"{filename_prefix}_{run}\")\n",
    "            model.plot_performance_comparison(save_filename=f\"{filename_prefix}_{run}\")\n",
    "\n",
    "        # Save the best model once after all runs\n",
    "        if best_model is not None:\n",
    "            # Use the last run number or a specific number if preferred\n",
    "            self.save_best_model(best_model, best_clf_name, filename_prefix, self.num_runs - 1)\n",
    "\n",
    "        # Convert metrics data to DataFrame\n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "        # Save metrics to CSV\n",
    "        metrics_df.to_csv(f\"./models/{filename_prefix}_cross_validation_metrics.csv\", index=False)\n",
    "        print(f\"Cross-validation metrics saved to ./models/{filename_prefix}_cross_validation_metrics.csv\")\n",
    "\n",
    "        # Concatenate all metric results\n",
    "        concatenated_metrics = pd.concat(metrics_list)\n",
    "\n",
    "        # Select only numeric columns for aggregation\n",
    "        numeric_columns = concatenated_metrics.select_dtypes(include=['number'])\n",
    "\n",
    "        # Aggregate only numeric columns\n",
    "        aggregated_metrics = numeric_columns.groupby(level=0).agg(['mean', 'std'])\n",
    "\n",
    "        # Combine non-numeric data with aggregated numeric data (if needed)\n",
    "        non_numeric_columns = concatenated_metrics.select_dtypes(exclude=['number']).drop_duplicates()\n",
    "        if not non_numeric_columns.empty:\n",
    "            aggregated_metrics = pd.concat([aggregated_metrics, non_numeric_columns], axis=1)\n",
    "\n",
    "        # Save aggregated metrics\n",
    "        aggregated_metrics.to_csv(f\"./models/{filename_prefix}_metrics_mean.csv\")\n",
    "        print(f\"Metrics saved to ./models/{filename_prefix}_metrics_mean.csv\")\n",
    "        \n",
    "        self.plot_metrics_altair(metrics_data)\n",
    "    \n",
    "    def save_best_model(self, model, clf_name, filename_prefix, run):\n",
    "        \"\"\"Save the best model based on selected metric.\"\"\"\n",
    "        joblib.dump(model, f'./models/{filename_prefix}_best_{clf_name}_{run}.joblib')\n",
    "        print(f\"Best model saved to ./models/{filename_prefix}_best_{clf_name}_{run}.joblib\")\n",
    "\n",
    "    def semi_supervised_learning(self):\n",
    "        \"\"\"Run semi-supervised learning on different dimensionality-reduced datasets.\"\"\"\n",
    "        data_list = {\n",
    "            \"pca\": self.data_combined_PCA,\n",
    "            \"tsne\": self.data_combined_tsne,\n",
    "            \"umap\": self.data_combined_UMAP\n",
    "        }\n",
    "        \n",
    "        for key, data in data_list.items():\n",
    "            # data[\"group\"] = data[\"group\"].replace({\n",
    "            #     'TRANSMEMBRANE PROTEINS:ALPHA-HELICAL': 0,\n",
    "            #     'TRANSMEMBRANE PROTEINS:BETA-BARREL': 1,\n",
    "            #     'MONOTOPIC MEMBRANE PROTEINS': 2\n",
    "            # })\n",
    "            \n",
    "            X_labeled, X_unlabeled, y_labeled, _ = train_test_split(\n",
    "                data[[\"Component 1\", \"Component 2\"]],\n",
    "                data[\"group\"], test_size=0.3,\n",
    "                stratify=data[\"group\"].to_list(),\n",
    "                random_state=self.random_state\n",
    "            )\n",
    "            \n",
    "            # Semi-supervised learning requires both labeled and unlabeled data\n",
    "            self.run_classification(X_labeled, y_labeled, ClassifierComparisonSemiSupervised, f\"semi_supervised_{key}\", X_unlabeled)\n",
    "\n",
    "        # Without Dimensionality Reduction\n",
    "        categorical_data = self.over_sampling_data_selected_feature_data[\"group\"]\n",
    "        # .replace({\n",
    "        #     'TRANSMEMBRANE PROTEINS:ALPHA-HELICAL': 0,\n",
    "        #     'TRANSMEMBRANE PROTEINS:BETA-BARREL': 1,\n",
    "        #     'MONOTOPIC MEMBRANE PROTEINS': 2\n",
    "        # })\n",
    "        \n",
    "        X_labeled, X_unlabeled, y_labeled, _ = train_test_split(\n",
    "            self.complete_numerical_data,\n",
    "            categorical_data, test_size=0.66,\n",
    "            stratify=categorical_data.to_list(),\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        self.run_classification(X_labeled, y_labeled, ClassifierComparisonSemiSupervised, \"semi_supervised_no_dr\", X_unlabeled)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def supervised_learning(self):\n",
    "        \"\"\"Run supervised learning on different dimensionality-reduced datasets.\"\"\"\n",
    "        data_list = {\n",
    "            \"pca\": self.data_combined_PCA,\n",
    "            \"tsne\": self.data_combined_tsne,\n",
    "            \"umap\": self.data_combined_UMAP\n",
    "        }\n",
    "        \n",
    "        for key, data in data_list.items():\n",
    "            X = data[[\"Component 1\", \"Component 2\"]]\n",
    "            y = data[\"group\"]\n",
    "            \n",
    "            self.run_classification(X, y, ClassifierComparison, f\"supervised_{key}\")\n",
    "        \n",
    "        # Without Dimensionality Reduction\n",
    "        X = self.complete_numerical_data\n",
    "        y = self.over_sampling_data_selected_feature_data[\"group\"]\n",
    "        self.run_classification(X, y, ClassifierComparison, \"supervised_no_dr\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "ml_job = MLJob()\n",
    "    \n",
    "# Run the steps sequentially\n",
    "ml_job.fix_missing_data()\\\n",
    "        .variable_separation()\\\n",
    "        .feature_selection()\\\n",
    "        .dimensionality_reduction()\\\n",
    "        .plot_charts()\\\n",
    "        .semi_supervised_learning()\\\n",
    "        .supervised_learning()\n",
    "\n",
    "print(\"Machine Learning Job completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cummulative Growth of Membrane Protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import random\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def generate_color_palette(start_color, end_color, num_colors):\n",
    "    # Convert hex colors to RGB\n",
    "    start_rgb = mcolors.hex2color(start_color)\n",
    "    end_rgb = mcolors.hex2color(end_color)\n",
    "\n",
    "    # Create a list of RGB colors in the gradient\n",
    "    colors = []\n",
    "    for i in range(num_colors):\n",
    "        r = start_rgb[0] + (end_rgb[0] - start_rgb[0]) * (i / (num_colors - 1))\n",
    "        g = start_rgb[1] + (end_rgb[1] - start_rgb[1]) * (i / (num_colors - 1))\n",
    "        b = start_rgb[2] + (end_rgb[2] - start_rgb[2]) * (i / (num_colors - 1))\n",
    "        colors.append((r, g, b))\n",
    "\n",
    "    # Convert RGB colors back to hex\n",
    "    hex_colors = [mcolors.rgb2hex(color) for color in colors]\n",
    "\n",
    "    return hex_colors\n",
    "\n",
    "\n",
    "title=\"page chart\"\n",
    "\n",
    "import_data = pd.read_csv(\"data/membrane_proteins.csv\")\n",
    "protein_db = import_data[import_data[\"is_master_protein\"] == \"MasterProtein\"]\n",
    "\n",
    "    # If not in cache, proceed with data retrieval and processing\n",
    "    # protein_db = get_table_as_dataframe(\"membrane_proteins\")\n",
    "\n",
    "\n",
    "d = pd.crosstab(protein_db.bibliography_year, columns=protein_db.group).cumsum()\n",
    "\n",
    "d = d.stack().reset_index()\n",
    "d = d.rename(columns={0:'CumulativeCount'})\n",
    "d = d.convert_dtypes()\n",
    "\n",
    "#### Line fit start here\n",
    "# Aggregate cumulative counts by year and group\n",
    "aggregated_df = d.groupby(['bibliography_year', 'group'])['CumulativeCount'].sum().reset_index()\n",
    "\n",
    "# Fit exponential model to the entire combined dataset\n",
    "combined_df = aggregated_df.groupby('bibliography_year')['CumulativeCount'].sum().reset_index()\n",
    "\n",
    "\n",
    "# Define exponential function for fitting\n",
    "def exp_growth(x, a, b):\n",
    "    return a * np.exp(b * (x - 2005))\n",
    "\n",
    "# Extract year and cumulative count from DataFrame\n",
    "years = combined_df['bibliography_year'].values\n",
    "cumulative_count = combined_df['CumulativeCount'].values\n",
    "\n",
    "# Use data from 1985 to 2004 to fit the exponential growth\n",
    "# years_fit = years[:15]  # 1985 to 2004\n",
    "# cumulative_count_fit = cumulative_count[:15]\n",
    "\n",
    "# Find the indices for years between 1985 and 2005 (inclusive)\n",
    "start_year = 1985\n",
    "end_year = 2005\n",
    "start_index = np.where(years == start_year)[0][0]\n",
    "end_index = np.where(years == end_year)[0][0] + 1  # Including 2005\n",
    "\n",
    "# Split the data for fitting\n",
    "years_fit = years[start_index:end_index]  # From 1985 to 2005\n",
    "cumulative_count_fit = cumulative_count[start_index:end_index]\n",
    "\n",
    "\n",
    "# Fit the exponential growth curve\n",
    "popt, _ = curve_fit(exp_growth, years_fit, cumulative_count_fit, p0=(100, 0.1))\n",
    "\n",
    "# Generate points for the fitted curve\n",
    "x_exp = np.arange(min(years), max(years) + 1)\n",
    "y_exp = exp_growth(x_exp, *popt)\n",
    "\n",
    "# Convert data to DataFrame\n",
    "df_fit = pd.DataFrame({\n",
    "    'Year': x_exp,\n",
    "    'Fitted Growth': y_exp\n",
    "})\n",
    "\n",
    "# Determine y-axis limit from the actual data\n",
    "y_max = cumulative_count.max()  # Set this to a bit above the max value for better visibility\n",
    "df_fit = df_fit[df_fit['Fitted Growth'] <= y_max]\n",
    "\n",
    "##### Line fit ends here+\n",
    "\n",
    "\n",
    "# Define a custom color palette\n",
    "start_color = '#005EB8'  # Red\n",
    "end_color = '#B87200'    # Green\n",
    "\n",
    "color_list = ['#D9DE84', '#005EB8', '#93C4F6', '#636B05']\n",
    "\n",
    "# Generate a color palette with 10 colors\n",
    "num_colors = len(list(protein_db['group'].unique()))\n",
    "palette = generate_color_palette(start_color, end_color, num_colors)\n",
    "random.shuffle(palette)\n",
    "\n",
    "custom_palette = alt.Scale(domain=list(protein_db['group'].unique()),\n",
    "                        range=color_list[:num_colors])\n",
    "bars = alt.Chart(d).mark_bar().encode(\n",
    "    x=alt.X('bibliography_year:O', title=\"Year\"),\n",
    "    y=alt.Y('CumulativeCount:Q', title = 'Entries'),\n",
    "    color=alt.Color('group', scale=custom_palette, legend=alt.Legend(title=\"Groups\", labelLimit=0, direction = 'vertical')),\n",
    "    tooltip=[alt.Tooltip('CumulativeCount:Q'),\n",
    "            alt.Tooltip('group'),\n",
    "            alt.Tooltip('bibliography_year:O')]\n",
    ")\n",
    "\n",
    "# Add exponential fit line\n",
    "line = alt.Chart(df_fit).mark_line(color='red', strokeDash=[5, 5]).encode(\n",
    "    x=alt.X('Year:O', title='Year'),\n",
    "    y=alt.Y('Fitted Growth:Q', title='Number of Structures', scale=alt.Scale(domain=[0, y_max])),\n",
    "    tooltip=['Year', 'Fitted Growth']\n",
    ")\n",
    "\n",
    "# Combine charts\n",
    "chart = bars + line\n",
    "\n",
    "# Customize chart appearance\n",
    "chart = chart.configure_axis(\n",
    "    labelFontSize=12,\n",
    "    titleFontSize=14\n",
    ").configure_title(\n",
    "    fontSize=16\n",
    ").configure_view(\n",
    "    stroke='transparent'\n",
    ").configure_legend(\n",
    "    orient='bottom',\n",
    "        # padding=20, \n",
    "        offset=2,\n",
    "        titleOrient='top',\n",
    "        labelLimit=0,\n",
    "    labelFontSize=12,\n",
    "    titleFontSize=14\n",
    ")\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiple Chart for MetaMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from src.services.Helpers.helper import (\n",
    "    generate_color_palette\n",
    ")\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def create_combined_chart(protein_db, title=\"Cumulative sum of resolved Membrane Protein (MP) Structures over time\", chart_width=800):\n",
    "    # Ensure 'group' column exists and update it\n",
    "    if 'group' not in protein_db.columns:\n",
    "        raise KeyError(\"The 'group' column is missing from the DataFrame.\")\n",
    "    \n",
    "    protein_db['group'] = protein_db['group'].replace({\n",
    "        'MONOTOPIC MEMBRANE PROTEINS': \"Group 1\",\n",
    "        'TRANSMEMBRANE PROTEINS:BETA-BARREL': \"Group 2\",\n",
    "        'TRANSMEMBRANE PROTEINS:ALPHA-HELICAL': \"Group 3\",\n",
    "    })\n",
    "    \n",
    "    # Define group labels and their meanings\n",
    "    group_labels = {\n",
    "        \"Group 1\": 'Group 1 (MONOTOPIC MEMBRANE PROTEINS)',\n",
    "        \"Group 2\": 'Group 2 (TRANSMEMBRANE PROTEINS:BETA-BARREL)',\n",
    "        \"Group 3\": 'Group 3 (TRANSMEMBRANE PROTEINS:ALPHA-HELICAL)'\n",
    "    }\n",
    "    \n",
    "    # Add a 'label' column for detailed legend information\n",
    "    protein_db['label'] = protein_db['group'].map(group_labels)\n",
    "    \n",
    "    \n",
    "    # Define a custom color palette\n",
    "    color_list = ['#D9DE84', '#93C4F6', '#005EB8']\n",
    "    unique_group_list = list(protein_db['label'].unique())\n",
    "    num_colors = len(unique_group_list)\n",
    "    ordered_list = [\n",
    "        'Group 1 (MONOTOPIC MEMBRANE PROTEINS)',\n",
    "        'Group 2 (TRANSMEMBRANE PROTEINS:BETA-BARREL)',\n",
    "        'Group 3 (TRANSMEMBRANE PROTEINS:ALPHA-HELICAL)'\n",
    "    ]\n",
    "    unique_group_list.sort(key=lambda x: ordered_list.index(x))\n",
    "    custom_palette = alt.Scale(domain=unique_group_list, range=color_list[:num_colors])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Check if 'label' column was successfully created\n",
    "    if 'label' not in protein_db.columns:\n",
    "        raise KeyError(\"The 'label' column was not created successfully.\")\n",
    "    \n",
    "    # Group by 'label' and count occurrences\n",
    "    grouped_data = protein_db.groupby([\"label\", \"group\"]).size().reset_index(name='CumulativeCount')\n",
    "    \n",
    "    # Group by 'taxonomic domain' and count occurrences\n",
    "    grouped_data_taxonomic_domain = protein_db[[\"taxonomic_domain\", \"label\", \"group\"]].groupby([\"taxonomic_domain\", \"label\", \"group\"]).size().reset_index(name=\"CumulativeCount\")\n",
    "    \n",
    "    # Sort by count\n",
    "    grouped_data = grouped_data.sort_values(by='CumulativeCount', ascending=True)\n",
    "\n",
    "    # Create a brush selection\n",
    "    brush = alt.selection_interval(encodings=['x', 'y'])\n",
    "    unique_group_list = list(protein_db['label'].unique())\n",
    "    \n",
    "    chart_width_2 = (chart_width - 100) / 2\n",
    "    \n",
    "    grouped_bar_chart = alt.Chart(grouped_data).mark_bar().encode(\n",
    "        x=alt.X(\n",
    "            'group:N', title='Group', sort=None, axis=alt.Axis(\n",
    "                labelAngle=0,\n",
    "                labelLimit=0\n",
    "            )\n",
    "        ),\n",
    "        y=alt.Y('CumulativeCount:Q', title='Cumulative MP Structures'),\n",
    "        color=alt.Color('label:N', scale=custom_palette, legend=alt.Legend(\n",
    "            title=\"Group\", \n",
    "            orient=\"bottom\", \n",
    "            labelLimit=0, \n",
    "            direction=\"vertical\"\n",
    "            )\n",
    "        ),\n",
    "        tooltip=[\"label\", \"CumulativeCount\"]\n",
    "    ).add_params(\n",
    "        brush\n",
    "    ).properties(\n",
    "        title=['Cumulative sum of resolved Membrane Protein (MP)', ' Structures categorized by group'],\n",
    "        width=chart_width_2\n",
    "    )\n",
    "    \n",
    "    grouped_bar_chart_taxonomic = alt.Chart(grouped_data_taxonomic_domain).mark_bar().encode(\n",
    "        x=alt.X(\n",
    "            'taxonomic_domain:N', title='Taxonomic Domain', sort=None, axis=alt.Axis(\n",
    "                labelAngle=0,\n",
    "                labelLimit=0\n",
    "            ), \n",
    "        ),\n",
    "        y=alt.Y('CumulativeCount:Q', title='Cumulative MP Structures'),\n",
    "        color=alt.Color('label:N', scale=custom_palette, legend=None),\n",
    "        tooltip=[\"label\", \"taxonomic_domain\", \"CumulativeCount\"]\n",
    "    ).transform_filter(\n",
    "        brush\n",
    "    ).properties(\n",
    "        title=['Cumulative sum of resolved Membrane Protein (MP)', ' Structures categorized by taxonomic domain'],\n",
    "        width=chart_width_2\n",
    "    ).interactive()\n",
    "    \n",
    "    # Prepare the data for the cumulative chart with exponential fit\n",
    "    d = pd.crosstab([protein_db[\"bibliography_year\"]], columns=[protein_db[\"group\"], protein_db[\"label\"]]).cumsum()\n",
    "\n",
    "    # Reset the index to convert 'bibliography_year' back to a column\n",
    "    d = d.reset_index()\n",
    "\n",
    "    # Melt the crosstab to get 'group' and 'label' as individual columns\n",
    "    d = d.melt(\n",
    "        id_vars=[\"bibliography_year\"], \n",
    "        var_name=[\"group\", \"label\"], \n",
    "        value_name=\"CumulativeCount\"\n",
    "    )\n",
    "    # Aggregate cumulative counts by year and group\n",
    "    aggregated_df = d.groupby(['bibliography_year', 'group'])['CumulativeCount'].sum().reset_index()\n",
    "    combined_df = aggregated_df.groupby('bibliography_year')['CumulativeCount'].sum().reset_index()\n",
    "    \n",
    "    # Fit exponential model\n",
    "    years = combined_df['bibliography_year'].values\n",
    "    cumulative_count = combined_df['CumulativeCount'].values\n",
    "    start_year, end_year = 1985, 2005\n",
    "    start_index = np.where(years == start_year)[0][0]\n",
    "    end_index = np.where(years == end_year)[0][0] + 1\n",
    "    years_fit = years[start_index:end_index]\n",
    "    cumulative_count_fit = cumulative_count[start_index:end_index]\n",
    "    popt, _ = curve_fit(exp_growth, years_fit, cumulative_count_fit, p0=(100, 0.1))\n",
    "    x_exp = np.arange(min(years), max(years) + 1)\n",
    "    y_exp = exp_growth(x_exp, *popt)\n",
    "    df_fit = pd.DataFrame({'Year': x_exp, 'Fitted Growth': y_exp})\n",
    "    y_max = cumulative_count.max()\n",
    "    df_fit = df_fit[df_fit['Fitted Growth'] <= y_max]\n",
    "    \n",
    "    entries_over_time = alt.Chart(d).mark_bar().encode(\n",
    "        x=alt.X('bibliography_year:O', title=\"Year\"),\n",
    "        y=alt.Y('CumulativeCount:Q', title='Cumulative MP Structures', scale=alt.Scale(domain=[0, y_max])),\n",
    "        color=alt.Color('label:N', scale=custom_palette, legend=None),\n",
    "        tooltip=[alt.Tooltip('CumulativeCount:Q'), alt.Tooltip('label'), alt.Tooltip('bibliography_year:O')]\n",
    "    ).transform_filter(\n",
    "        brush\n",
    "    ).properties(\n",
    "        width=chart_width,\n",
    "        title=\"Cumulative sum of resolved Membrane Protein (MP) Structures over time\"\n",
    "    )\n",
    "    \n",
    "    # Add exponential fit line\n",
    "    line = alt.Chart(df_fit).mark_line(color='red', strokeDash=[5, 5]).encode(\n",
    "        x=alt.X('Year:O', title='Year'),\n",
    "        y=alt.Y('Fitted Growth:Q', title='Cumulative MP Structures', scale=alt.Scale(domain=[0, y_max])),\n",
    "        tooltip=['Year', 'Fitted Growth']\n",
    "    )\n",
    "    # Combine the bar chart and the exponential fit line\n",
    "    chart_with_regression = entries_over_time + line\n",
    "    group_1 = alt.hconcat(grouped_bar_chart, grouped_bar_chart_taxonomic)\n",
    "    \n",
    "    # Combine both charts into one visualization\n",
    "    combined_chart = alt.vconcat(group_1, chart_with_regression).configure_view(\n",
    "        stroke='transparent'\n",
    "    ).configure_legend(\n",
    "        orient='bottom',\n",
    "        offset=2,\n",
    "        titleOrient='top',\n",
    "        labelLimit=0\n",
    "    )\n",
    "\n",
    "    return combined_chart\n",
    "\n",
    "def exp_growth(x, a, b):\n",
    "    return a * np.exp(b * (x - 2005))\n",
    "\n",
    "\n",
    "grouped_chart = create_combined_chart(all_data)\n",
    "grouped_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiple Chart Display Horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_df_db = import_data\n",
    "\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def create_combined_chart_cumulative_growth(protein_db, chart_width=1000):\n",
    "    # Ensure 'group' column exists and update it\n",
    "    if 'group' not in protein_db.columns:\n",
    "        raise KeyError(\"The 'group' column is missing from the DataFrame.\")\n",
    "    \n",
    "    protein_db['group'] = protein_db['group'].replace({\n",
    "        'MONOTOPIC MEMBRANE PROTEINS': \"Group 1\",\n",
    "        'TRANSMEMBRANE PROTEINS:BETA-BARREL': \"Group 2\",\n",
    "        'TRANSMEMBRANE PROTEINS:ALPHA-HELICAL': \"Group 3\",\n",
    "    })\n",
    "    \n",
    "    # Define group labels and their meanings\n",
    "    group_labels = {\n",
    "        \"Group 1\": 'Group 1 (MONOTOPIC MEMBRANE PROTEINS)',\n",
    "        \"Group 2\": 'Group 2 (TRANSMEMBRANE PROTEINS:BETA-BARREL)',\n",
    "        \"Group 3\": 'Group 3 (TRANSMEMBRANE PROTEINS:ALPHA-HELICAL)'\n",
    "    }\n",
    "    \n",
    "    # Add a 'label' column for detailed legend information\n",
    "    protein_db['label'] = protein_db['group'].map(group_labels)\n",
    "    \n",
    "    \n",
    "    # Define a custom color palette\n",
    "    color_list = ['#D9DE84', '#93C4F6', '#005EB8']\n",
    "    unique_group_list = list(protein_db['label'].unique())\n",
    "    num_colors = len(unique_group_list)\n",
    "    ordered_list = [\n",
    "        'Group 1 (MONOTOPIC MEMBRANE PROTEINS)',\n",
    "        'Group 2 (TRANSMEMBRANE PROTEINS:BETA-BARREL)',\n",
    "        'Group 3 (TRANSMEMBRANE PROTEINS:ALPHA-HELICAL)'\n",
    "    ]\n",
    "    unique_group_list.sort(key=lambda x: ordered_list.index(x))\n",
    "    custom_palette = alt.Scale(domain=unique_group_list, range=color_list[:num_colors])\n",
    "    \n",
    "    \n",
    "    unique_group_list_group = list(protein_db['group'].unique())\n",
    "    num_colors_group = len(unique_group_list_group)\n",
    "    ordered_list_group = [\n",
    "        'Group 1',\n",
    "        'Group 2',\n",
    "        'Group 3'\n",
    "    ]\n",
    "    unique_group_list_group.sort(key=lambda x: ordered_list_group.index(x))\n",
    "    custom_palette_group = alt.Scale(domain=unique_group_list_group, range=color_list[:num_colors_group])\n",
    "    \n",
    "    \n",
    "    # Check if 'label' column was successfully created\n",
    "    if 'label' not in protein_db.columns:\n",
    "        raise KeyError(\"The 'label' column was not created successfully.\")\n",
    "    \n",
    "    # Group by 'label' and count occurrences\n",
    "    grouped_data = protein_db.groupby([\"label\", \"group\"]).size().reset_index(name='CumulativeCount')\n",
    "    \n",
    "    # Group by 'taxonomic domain' and count occurrences\n",
    "    grouped_data_taxonomic_domain = protein_db[[\"taxonomic_domain\", \"label\", \"group\"]].groupby([\"taxonomic_domain\", \"label\", \"group\"]).size().reset_index(name=\"CumulativeCount\")\n",
    "    \n",
    "    # Sort by count\n",
    "    grouped_data = grouped_data.sort_values(by='CumulativeCount', ascending=True)\n",
    "\n",
    "\n",
    "    #############################3rd Chart###########################\n",
    "    #################################################################\n",
    "    \n",
    "    # Prepare the data for the cumulative chart with exponential fit\n",
    "    d = pd.crosstab([protein_db[\"bibliography_year\"]], columns=[protein_db[\"group\"], protein_db[\"label\"]]).cumsum()\n",
    "\n",
    "    # Reset the index to convert 'bibliography_year' back to a column\n",
    "    d = d.reset_index()\n",
    "\n",
    "    # Melt the crosstab to get 'group' and 'label' as individual columns\n",
    "    d = d.melt(\n",
    "        id_vars=[\"bibliography_year\"], \n",
    "        var_name=[\"group\", \"label\"], \n",
    "        value_name=\"CumulativeCount\"\n",
    "    )\n",
    "    \n",
    "    aggregated_df = d.groupby(['bibliography_year', 'label'])['CumulativeCount'].sum().reset_index()\n",
    "    combined_df = aggregated_df.groupby('bibliography_year')['CumulativeCount'].sum().reset_index()\n",
    "    \n",
    "    # Fit exponential model\n",
    "    years = combined_df['bibliography_year'].values\n",
    "    cumulative_count = combined_df['CumulativeCount'].values\n",
    "    start_year, end_year = 1985, 2005\n",
    "    start_index = np.where(years == start_year)[0][0]\n",
    "    end_index = np.where(years == end_year)[0][0] + 1\n",
    "    years_fit = years[start_index:end_index]\n",
    "    cumulative_count_fit = cumulative_count[start_index:end_index]\n",
    "    popt, _ = curve_fit(exp_growth, years_fit, cumulative_count_fit, p0=(100, 0.1))\n",
    "    x_exp = np.arange(min(years), max(years) + 1)\n",
    "    y_exp = exp_growth(x_exp, *popt)\n",
    "    df_fit = pd.DataFrame({'Year': x_exp, 'Fitted Growth': y_exp})\n",
    "    y_max = cumulative_count.max()\n",
    "    df_fit = df_fit[df_fit['Fitted Growth'] <= y_max]\n",
    "    \n",
    "    # Create a brush selection\n",
    "    brush = alt.selection_interval(encodings=['x', 'y'])\n",
    "    unique_group_list = list(protein_db['label'].unique())\n",
    "    \n",
    "    # Calculate the available width by subtracting the padding\n",
    "    padding = 200 \n",
    "    available_width = chart_width - padding\n",
    "\n",
    "    chart_width_1 = 0.5*available_width\n",
    "    chart_width_2 = 0.5*chart_width_1\n",
    "    \n",
    "    grouped_bar_chart = alt.Chart(grouped_data).mark_bar().encode(\n",
    "        x=alt.X(\n",
    "            'group:N', title='Group', sort=None, axis=alt.Axis(\n",
    "                labelAngle=0,\n",
    "                labelLimit=0\n",
    "            )\n",
    "        ),\n",
    "        y=alt.Y('CumulativeCount:Q', title='Cumulative MP Structures', scale=alt.Scale(domain=[0, y_max])),\n",
    "        color=alt.Color('label:N', scale=custom_palette, legend=alt.Legend(\n",
    "            title=\"Group\", \n",
    "            orient=\"bottom\", \n",
    "            labelLimit=0, \n",
    "            direction=\"vertical\"\n",
    "            )\n",
    "        ),\n",
    "        tooltip=[\"label\", \"CumulativeCount\"]\n",
    "    ).add_params(\n",
    "        brush\n",
    "    ).properties(\n",
    "        title=['Cumulative resolved MPs', ' categorized by group'],\n",
    "        width=chart_width_2\n",
    "    )\n",
    "    \n",
    "    grouped_bar_chart_taxonomic = alt.Chart(grouped_data_taxonomic_domain).mark_bar().encode(\n",
    "        x=alt.X(\n",
    "            'taxonomic_domain:N', title='Taxonomic Domain', sort=None, axis=alt.Axis(\n",
    "                labelAngle=15,\n",
    "                labelLimit=0\n",
    "            ), \n",
    "        ),\n",
    "        y=alt.Y('CumulativeCount:Q', title=None, scale=alt.Scale(domain=[0, y_max])),\n",
    "        color=alt.Color('label:N', scale=custom_palette, legend=None),\n",
    "        tooltip=[\"label\", \"taxonomic_domain\", \"CumulativeCount\"]\n",
    "    ).transform_filter(\n",
    "        brush\n",
    "    ).properties(\n",
    "        title=['Cumulative resolved MPs ', 'categorized by taxonomic domain'],\n",
    "        width=chart_width_2\n",
    "    ).interactive()\n",
    "    \n",
    "    \n",
    "    entries_over_time = alt.Chart(d).mark_bar().encode(\n",
    "        x=alt.X('bibliography_year:O', title=\"Year\"),\n",
    "        y=alt.Y('CumulativeCount:Q', title=None, scale=alt.Scale(domain=[0, y_max])),\n",
    "        color=alt.Color('label:N', scale=custom_palette, legend=None),\n",
    "        tooltip=[alt.Tooltip('CumulativeCount:Q'), alt.Tooltip('label'), alt.Tooltip('bibliography_year:O')]\n",
    "    ).transform_filter(\n",
    "        brush\n",
    "    ).properties(\n",
    "        width=chart_width_1,\n",
    "        title=[\"Cumulative resolved Membrane \", \"Protein (MP) Structures over time\"]\n",
    "    ).interactive()\n",
    "    \n",
    "    # Add exponential fit line\n",
    "    line = alt.Chart(df_fit).mark_line(color='red', strokeDash=[5, 5]).encode(\n",
    "        x=alt.X('Year:O', title='Year'),\n",
    "        y=alt.Y('Fitted Growth:Q', title='Cumulative MP Structures', scale=alt.Scale(domain=[0, y_max])),\n",
    "        tooltip=['Year', 'Fitted Growth']\n",
    "    )\n",
    "    # Combine the bar chart and the exponential fit line\n",
    "    chart_with_regression = entries_over_time + line\n",
    "    \n",
    "    # Combine both charts into one visualization\n",
    "    combined_chart = alt.hconcat(\n",
    "        grouped_bar_chart, \n",
    "        grouped_bar_chart_taxonomic,  \n",
    "        chart_with_regression\n",
    "    ).configure_view(\n",
    "        stroke='transparent'\n",
    "    ).configure_legend(\n",
    "        orient='bottom',\n",
    "        offset=2,\n",
    "        titleOrient='top',\n",
    "        labelLimit=0\n",
    "    )\n",
    "\n",
    "    return combined_chart\n",
    "\n",
    "def exp_growth(x, a, b):\n",
    "    return a * np.exp(b * (x - 2005))\n",
    "\n",
    "\n",
    "grouped_chart = create_combined_chart_cumulative_growth(result_df_db)\n",
    "grouped_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Custom Function for multiple Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def exp_growth(x, a, b):\n",
    "    return a * np.exp(b * (x - 20))\n",
    "\n",
    "\n",
    "def extend_dataframe_with_missing_years(df, groups):\n",
    "    \"\"\"\n",
    "    Extend the dataframe to include missing years with specified groups and labels.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The original dataframe with at least 'bibliography_year' column.\n",
    "    groups (dict): A dictionary where keys are group identifiers and values are group labels.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The extended dataframe including missing years and group information.\n",
    "    \"\"\"\n",
    "    # Identify existing years in the dataframe\n",
    "    existing_years = df['bibliography_year'].unique()\n",
    "    min_year, max_year = min(existing_years), max(existing_years)\n",
    "    all_years = set(range(min_year, max_year + 1))\n",
    "    missing_years = all_years - set(existing_years)\n",
    "    \n",
    "    # Create new entries for missing years\n",
    "    new_entries = []\n",
    "    for year in missing_years:\n",
    "        for group, label in groups.items():\n",
    "            new_entries.append({\n",
    "                'bibliography_year': year,\n",
    "                'group': group,\n",
    "                'label': label,\n",
    "                'other_data': None,\n",
    "                'CumulativeCount': 0\n",
    "            })\n",
    "    \n",
    "    # Convert new entries to DataFrame\n",
    "    missing_df = pd.DataFrame(new_entries)\n",
    "    \n",
    "    # Combine the original DataFrame with the new entries\n",
    "    df_extended = pd.concat([df, missing_df], ignore_index=True)\n",
    "    \n",
    "    # Sort by 'bibliography_year' if needed\n",
    "    df_extended = df_extended.sort_values(by='bibliography_year').reset_index(drop=True)\n",
    "    \n",
    "    return df_extended\n",
    "\n",
    "\n",
    "def create_combined_chart_cumulative_growths(protein_db, chart_width=1000):\n",
    "    # Ensure 'group' column exists and update it\n",
    "    if 'group' not in protein_db.columns:\n",
    "        raise KeyError(\"The 'group' column is missing from the DataFrame.\")\n",
    "    \n",
    "    protein_db['group'] = protein_db['group'].replace({\n",
    "        'MONOTOPIC MEMBRANE PROTEINS': \"Group 1\",\n",
    "        'TRANSMEMBRANE PROTEINS:BETA-BARREL': \"Group 2\",\n",
    "        'TRANSMEMBRANE PROTEINS:ALPHA-HELICAL': \"Group 3\",\n",
    "    })\n",
    "    \n",
    "    # Define group labels and their meanings\n",
    "    group_labels = {\n",
    "        \"Group 1\": 'Group 1 (MONOTOPIC MEMBRANE PROTEINS)',\n",
    "        \"Group 2\": 'Group 2 (TRANSMEMBRANE PROTEINS:BETA-BARREL)',\n",
    "        \"Group 3\": 'Group 3 (TRANSMEMBRANE PROTEINS:ALPHA-HELICAL)'\n",
    "    }\n",
    "    \n",
    "    # Add a 'label' column for detailed legend information\n",
    "    protein_db['label'] = protein_db['group'].map(group_labels)\n",
    "    \n",
    "    # Define a custom color palette\n",
    "    color_list = ['#D9DE84', '#93C4F6', '#005EB8']\n",
    "    unique_group_list = list(protein_db['label'].unique())\n",
    "    num_colors = len(unique_group_list)\n",
    "    ordered_list = [\n",
    "        'Group 1 (MONOTOPIC MEMBRANE PROTEINS)',\n",
    "        'Group 2 (TRANSMEMBRANE PROTEINS:BETA-BARREL)',\n",
    "        'Group 3 (TRANSMEMBRANE PROTEINS:ALPHA-HELICAL)'\n",
    "    ]\n",
    "    unique_group_list.sort(key=lambda x: ordered_list.index(x))\n",
    "    custom_palette = alt.Scale(domain=unique_group_list, range=color_list[:num_colors])\n",
    "    \n",
    "    # Check if 'label' column was successfully created\n",
    "    if 'label' not in protein_db.columns:\n",
    "        raise KeyError(\"The 'label' column was not created successfully.\")\n",
    "    \n",
    "    # Group by 'label' and count occurrences\n",
    "    grouped_data = protein_db.groupby([\"label\", \"group\"]).size().reset_index(name='CumulativeCount')\n",
    "    \n",
    "    # Group by 'taxonomic domain' and count occurrences\n",
    "    grouped_data_taxonomic_domain = protein_db[[\"taxonomic_domain\", \"label\", \"group\"]].groupby([\"taxonomic_domain\", \"label\", \"group\"]).size().reset_index(name=\"CumulativeCount\")\n",
    "    \n",
    "    # Prepare the data for the cumulative chart with exponential fit\n",
    "    \n",
    "    # Extend dataframe\n",
    "    d = pd.crosstab([protein_db[\"bibliography_year\"]], columns=[protein_db[\"group\"], protein_db[\"label\"]]).cumsum()\n",
    "    d = d.reset_index()\n",
    "    d = d.melt(\n",
    "        id_vars=[\"bibliography_year\"], \n",
    "        var_name=[\"group\", \"label\"], \n",
    "        value_name=\"CumulativeCount\"\n",
    "    )\n",
    "    d = extend_dataframe_with_missing_years(d, group_labels)\n",
    "    # Convert bibliography_year to an index\n",
    "    d['index'] = pd.factorize(d['bibliography_year'])[0]\n",
    "    \n",
    "    aggregated_df = d.groupby(['index', 'label', 'bibliography_year'])['CumulativeCount'].sum().reset_index()\n",
    "    combined_df = aggregated_df.groupby(['index', 'bibliography_year'])['CumulativeCount'].sum().reset_index()\n",
    "    \n",
    "    # Fit exponential model\n",
    "    indices = combined_df['index'].values\n",
    "    cumulative_count = combined_df['CumulativeCount'].values\n",
    "    \n",
    "    start_year, end_year = 0, 20\n",
    "    start_index = np.where(indices == start_year)[0][0]\n",
    "    end_index = np.where(indices == end_year)[0][0] + 1\n",
    "    years_fit = indices[start_index:end_index]\n",
    "    cumulative_count_fit = cumulative_count[start_index:end_index]\n",
    "    popt, _ = curve_fit(exp_growth, years_fit, cumulative_count_fit, p0=(100, 0.1))\n",
    "    x_exp = np.arange(min(indices), max(indices) + 1)\n",
    "    y_exp = exp_growth(x_exp, *popt)\n",
    "    \n",
    "    df_fit = pd.DataFrame({'Index': x_exp, 'Fitted Growth': y_exp})\n",
    "    y_max = cumulative_count.max()\n",
    "    df_fit = df_fit[df_fit['Fitted Growth'] <= y_max]\n",
    "    \n",
    "    # Create a brush selection\n",
    "    brush = alt.selection_interval(encodings=['x', 'y'])\n",
    "    unique_group_list = list(protein_db['label'].unique())\n",
    "    \n",
    "    # Calculate the available width by subtracting the padding\n",
    "    padding = 200 \n",
    "    available_width = chart_width - padding\n",
    "\n",
    "    chart_width_1 = 0.5*available_width\n",
    "    chart_width_2 = 0.5*chart_width_1\n",
    "    \n",
    "    grouped_bar_chart = alt.Chart(grouped_data).mark_bar().encode(\n",
    "        x=alt.X(\n",
    "            'group:N', title='Group', sort=None, axis=alt.Axis(\n",
    "                labelAngle=0,\n",
    "                labelLimit=0\n",
    "            )\n",
    "        ),\n",
    "        y=alt.Y('CumulativeCount:Q', title='Cumulative MP Structures', scale=alt.Scale(domain=[0, y_max])),\n",
    "        color=alt.Color('label:N', scale=custom_palette, legend=alt.Legend(\n",
    "            title=\"Group\", \n",
    "            orient=\"bottom\", \n",
    "            labelLimit=0, \n",
    "            direction=\"vertical\"\n",
    "            )\n",
    "        ),\n",
    "        tooltip=[\"label\", \"CumulativeCount\"]\n",
    "    ).add_params(\n",
    "        brush\n",
    "    ).properties(\n",
    "        title=['Cumulative resolved MPs', ' categorized by group'],\n",
    "        width=chart_width_2\n",
    "    )\n",
    "    \n",
    "    grouped_bar_chart_taxonomic = alt.Chart(grouped_data_taxonomic_domain).mark_bar().encode(\n",
    "        x=alt.X(\n",
    "            'taxonomic_domain:N', title='Taxonomic Domain', sort=None, axis=alt.Axis(\n",
    "                labelAngle=15,\n",
    "                labelLimit=0\n",
    "            ), \n",
    "        ),\n",
    "        y=alt.Y('CumulativeCount:Q', title=None, scale=alt.Scale(domain=[0, y_max])),\n",
    "        color=alt.Color('label:N', scale=custom_palette, legend=None),\n",
    "        tooltip=[\"label\", \"taxonomic_domain\", \"CumulativeCount\"]\n",
    "    ).transform_filter(\n",
    "        brush\n",
    "    ).properties(\n",
    "        title=['Cumulative resolved MPs ', 'categorized by taxonomic domain'],\n",
    "        width=chart_width_2\n",
    "    ).interactive()\n",
    "    \n",
    "    entries_over_time = alt.Chart(d).mark_bar().encode(\n",
    "        x=alt.X('index:Q', title=\"Year since first structure (1985)\", axis=alt.Axis(labelAngle=0, tickCount=6),\n",
    "            scale=alt.Scale(domain=[0, d['index'].max()])\n",
    "        ),\n",
    "        y=alt.Y('CumulativeCount:Q', title=None, \n",
    "                scale=alt.Scale(domain=[0, y_max])\n",
    "        ),\n",
    "        color=alt.Color('label:N', scale=custom_palette, legend=None),\n",
    "        tooltip=[alt.Tooltip('bibliography_year:O'), alt.Tooltip('CumulativeCount:Q'), alt.Tooltip('label'), alt.Tooltip('index:O')]\n",
    "    ).transform_filter(\n",
    "        brush\n",
    "    ).properties(\n",
    "        width=chart_width_1,\n",
    "        title=[\"Cumulative resolved Membrane \", \"Protein (MP) Structures over time\"]\n",
    "    ).interactive()\n",
    "    \n",
    "    # Add exponential fit line\n",
    "    line = alt.Chart(df_fit).mark_line(color='red', strokeDash=[5, 5]).encode(\n",
    "        x=alt.X('Index:Q', title=\"Year since first structure (1985)\", axis=alt.Axis(labelAngle=0, tickCount=6),\n",
    "            scale=alt.Scale(domain=[0, d['index'].max()])\n",
    "        ),\n",
    "        y=alt.Y('Fitted Growth:Q', title='Cumulative MP Structures', \n",
    "                scale=alt.Scale(domain=[0, y_max])\n",
    "            ),\n",
    "        tooltip=['Index', 'Fitted Growth']\n",
    "    )\n",
    "    # Combine the bar chart and the exponential fit line\n",
    "    chart_with_regression = entries_over_time + line\n",
    "    \n",
    "    # Combine both charts into one visualization\n",
    "    combined_chart = alt.hconcat(\n",
    "        grouped_bar_chart, \n",
    "        grouped_bar_chart_taxonomic,  \n",
    "        chart_with_regression\n",
    "    ).configure_view(\n",
    "        stroke='transparent'\n",
    "    ).configure_legend(\n",
    "        orient='bottom',\n",
    "        offset=2,\n",
    "        titleOrient='top',\n",
    "        labelLimit=0\n",
    "    )\n",
    "\n",
    "    return combined_chart\n",
    "\n",
    "\n",
    "grouped_chart = create_combined_chart_cumulative_growths(protein_db)\n",
    "grouped_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View Discrepancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.Training.services import aggregate_inconsistencies, transform_dataframe\n",
    "\n",
    "def create_visualization(data, chart_width=None):\n",
    "    \"\"\"\n",
    "        Creates a visualization of inconsistencies by year and protein type.\n",
    "\n",
    "        Args:\n",
    "        - data (pd.DataFrame): The input dataframe.\n",
    "        - chart_width (int): The width of the chart and table.\n",
    "\n",
    "        Returns:\n",
    "        - alt.Chart: The combined Altair chart and table.\n",
    "    \"\"\"\n",
    "    # Define selections\n",
    "    brush = alt.selection_interval(encodings=[\"x\", \"y\"])\n",
    "    click = alt.selection_point(fields=['inconsistencies'], name='click')\n",
    "\n",
    "    # Check and set chart width\n",
    "    if chart_width and isinstance(chart_width,int):\n",
    "        chart_width = int(chart_width) - 50\n",
    "    else:\n",
    "        chart_width = 800\n",
    "\n",
    "    # Create line chart\n",
    "    line_chart = alt.Chart(data).mark_line(point=True, interpolate='monotone').encode(\n",
    "        x=alt.X('bibliography_year:O', title=\"Year\"),\n",
    "        y=alt.Y(\n",
    "            'inconsistencies:Q', \n",
    "            title=\"Inconsistencies\",\n",
    "            scale=alt.Scale(domain=(0, data['inconsistencies'].max() * 1.1))  # Added a small buffer to y-axis\n",
    "        ),\n",
    "        tooltip=[\n",
    "            'bibliography_year', 'inconsistencies', \n",
    "            'protein_codes', 'group (OPM)', \n",
    "            'group (MPstruc)'\n",
    "        ]\n",
    "    ).add_params(\n",
    "        brush, click\n",
    "    ).properties(\n",
    "        width=chart_width,\n",
    "        title='Discrepancies in membrane protein structure groups observed over time using the OPM and MPstruc databases.',\n",
    "    )\n",
    "    \n",
    "    # Create table\n",
    "    table = alt.Chart(data).mark_text(align='left').encode(\n",
    "        y=alt.Y('row_number:O', axis=None),\n",
    "    ).transform_filter(\n",
    "        brush\n",
    "    ).transform_filter(\n",
    "        click\n",
    "    ).transform_window(\n",
    "        row_number='row_number()'\n",
    "    ).transform_filter(\n",
    "        'datum.row_number < 15'\n",
    "    )\n",
    "\n",
    "    width_array = [0, 0, 0, 0, 0]\n",
    "    \n",
    "    # Create individual columns\n",
    "    pdb_code = table.encode(\n",
    "        text='pdb_code:N'\n",
    "    ).properties(\n",
    "        width=width_array[0],\n",
    "        title=alt.TitleParams(text='PDB Code', align='left')\n",
    "    )\n",
    "\n",
    "    group = table.encode(\n",
    "        text='group (MPstruc):N'\n",
    "    ).properties(\n",
    "        width=width_array[1],\n",
    "        title=alt.TitleParams(text='Group (MPstruc)', align='left')\n",
    "    )\n",
    "\n",
    "    OPM_group = table.encode(\n",
    "        text='group (OPM):N'\n",
    "    ).properties(\n",
    "        width=width_array[2],\n",
    "        title=alt.TitleParams(text='Group (OPM)', align='left')\n",
    "    )\n",
    "\n",
    "    year = table.encode(\n",
    "        text='bibliography_year:N'\n",
    "    ).properties(\n",
    "        width=width_array[3],\n",
    "        title=alt.TitleParams(text='Year', align='left')\n",
    "    )\n",
    "\n",
    "    method = table.encode(\n",
    "        text='experimental_method:N'\n",
    "    ).properties(\n",
    "        width=width_array[4],\n",
    "        title=alt.TitleParams(text='Experimental Method', align='left')\n",
    "    )\n",
    "    \n",
    "    # Concatenate columns horizontally\n",
    "    table_layout = alt.hconcat(\n",
    "        pdb_code, group, OPM_group, year, method\n",
    "    ).resolve_legend(\n",
    "        color=\"independent\"\n",
    "    )\n",
    "\n",
    "    # Concatenate chart and table vertically\n",
    "    chart_with_table = alt.vconcat(\n",
    "        line_chart, table_layout\n",
    "    ).configure_view(\n",
    "        strokeWidth=0\n",
    "    )\n",
    "\n",
    "    return chart_with_table\n",
    "\n",
    "# Load the data again to avoid manupulations made above\n",
    "all_data, uniprot, opm, mp_and_pdb = load_data()\n",
    "\n",
    "# Prepare combined DataFrame\n",
    "df_combined = all_data[[\n",
    "    \"pdb_code_opm\", \"famsupclasstype_type_name\", \n",
    "    \"family_superfamily_classtype_name\", \n",
    "    \"group\", \"bibliography_year\", \n",
    "    \"rcsentinfo_experimental_method\"\n",
    "]].copy()\n",
    "\n",
    "# Drop rows with any NaN values\n",
    "df_combined.dropna(inplace=True)\n",
    "# Aggregate inconsistencies\n",
    "inconsistencies_by_year = aggregate_inconsistencies(df_combined, pdb_code_column=\"pdb_code_opm\")\n",
    "# Transform the aggregated data\n",
    "transformed_data = transform_dataframe(inconsistencies_by_year)\n",
    "print(inconsistencies_by_year)\n",
    "chart_with_table = create_visualization(transformed_data)\n",
    "chart_with_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other Interesting Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.package import separate_numerical_categorical\n",
    "from src.Jobs.transformData import report_and_clean_missing_values\n",
    "\n",
    "\n",
    "data = clean_df\n",
    "\n",
    "data = data.dropna()\n",
    "numerical_cols, categorical_cols = separate_numerical_categorical(data)\n",
    "numerical_data = data[numerical_cols]\n",
    "categorical_data = data[categorical_cols]\n",
    "\n",
    "complete_data = pd.concat([complete_numerical_data, categorical_data], axis=1)\n",
    "# Calculate the counts of occurrences of each group-subgroup pair\n",
    "group_subgroup_counts = complete_data.groupby(['group', 'subgroup']).size().reset_index(name='count')\n",
    "\n",
    "group_subgroup_counts_df = pd.DataFrame(group_subgroup_counts).sort_values(by='count', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_numerical_data = pd.concat([numerical_data, onehot_data], axis=1)\n",
    "complete_data = pd.concat([complete_numerical_data, categorical_data], axis=1)\n",
    "count_by_category = complete_data['subgroup'].value_counts().reset_index()\n",
    "count_by_category.columns = ['subgroup', 'count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tree Map for Subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import squarify\n",
    "\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Create a color scale based on count values\n",
    "norm = plt.Normalize(count_by_category['count'].min(), count_by_category['count'].max())\n",
    "colors = cm.Reds(norm(count_by_category['count']))\n",
    "\n",
    "# Plot the tree map with color gradients\n",
    "plt.figure(figsize=(10, 6))\n",
    "squarify.plot(sizes=count_by_category['count'], label=count_by_category['count'], color=colors, alpha=0.7)\n",
    "plt.axis('off')\n",
    "plt.title('Tree Map of Subgroups with Counts')\n",
    "#plt.show()\n",
    "plt.savefig('tree_map_high_quality.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sunburst for MP subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# Create a sunburst chart\n",
    "fig = px.sunburst(group_subgroup_counts_df, path=['group', 'subgroup', 'count'], values='count', color='count', color_continuous_scale='blues', width=800, height=500)\n",
    "fig.update_layout(title_text='Sunburst Chart of Membrane Protein Sub-group.')\n",
    "fig.write_image(\"sunburst_chart_high_dimensional.png\", width=1800, height=1200, scale=2)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interactive Tree Map for Subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "fig = px.treemap(group_subgroup_counts_df, path=[\"group\", 'subgroup', 'count'], values='count',\n",
    "    color='group', hover_data=[\"group\", 'subgroup', 'count'],\n",
    "    color_discrete_map={'MONOTOPIC MEMBRANE PROTEINS':'#2ca25f', 'TRANSMEMBRANE PROTEINS:ALPHA-HELICAL':'#fdc086', 'TRANSMEMBRANE PROTEINS:BETA-BARREL':'#beaed4'},\n",
    "    color_continuous_midpoint=np.average(group_subgroup_counts_df['count'], weights=group_subgroup_counts_df['count']),\n",
    "    \n",
    "    )\n",
    "fig.update_traces(root_color=\"lightgrey\", )\n",
    "fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\n",
    "fig.write_image(\"hierarchical_tree_map.png\", scale=3)  #\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cumulative sum of resolved Membrane Protein (MP) Structures over time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.services.pages import Pages\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "pages = Pages(mp_and_pdb)\n",
    "data_map, map= pages.getMap()\n",
    "map_chart = alt.Chart.from_dict(map)\n",
    "map_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a DataFrame from the provided data\n",
    "map_df = pd.DataFrame(data_map)\n",
    "\n",
    "# Create the chart using Altair\n",
    "chart_bar = alt.Chart(map_df).transform_filter(\n",
    "    alt.datum.count > 0\n",
    ").mark_bar().encode(\n",
    "    x='country:O',\n",
    "    y='count:Q',\n",
    "    color=alt.Color(\"count:Q\", \n",
    "        scale=alt.Scale(domain=[0, 2000]), \n",
    "        legend=alt.Legend(\n",
    "            title=\"No. of released Membrane Protein Structure\", \n",
    "            orient=\"bottom\", titleLimit=2000\n",
    "        )\n",
    "    ),\n",
    "    tooltip=alt.Tooltip('count:Q', title=\"No. of released Membrane Protein Structure\")\n",
    ").properties(\n",
    "    title='Resolved Membrane Protein (MP) Structures across different countries.',\n",
    "    width=\"container\"\n",
    ")\n",
    "\n",
    "chart_bar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### US Contribution Against Other Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "data = map_df[(map_df['count'] > 0) & map_df['country'].notna() & (map_df['country'] != 'None')]\n",
    "us_uk_data = data[data['iso_code_2'].isin(['US', 'GB'])]\n",
    "others_data = data[~data['iso_code_2'].isin(['US', 'GB'])]\n",
    "\n",
    "# Summing US and UK, and preparing individual entries for other countries\n",
    "grouped_us_uk = us_uk_data.groupby('iso_code_2').sum().reset_index()\n",
    "grouped_others = others_data.groupby('iso_code_2').sum().reset_index()\n",
    "\n",
    "# Setting a common group for stacking in visualization\n",
    "grouped_us_uk['Group'] = 'US and UK'\n",
    "grouped_others['Group'] = 'Others'\n",
    "\n",
    "# Concatenating the results\n",
    "vis_data = pd.concat([grouped_us_uk, grouped_others], ignore_index=True)\n",
    "\n",
    "# Visualization\n",
    "chart = alt.Chart(vis_data).mark_bar().encode(\n",
    "    y='sum(count):Q',\n",
    "    x=alt.Y('Group:N', title='Group'),\n",
    "    #color=alt.Color('iso_code_2:N',legend=alt.Legend(title=\"Country\")),\n",
    "    tooltip=[alt.Tooltip('iso_code_2:N', title='Country'), alt.Tooltip('sum(count):Q', title='Total Count')]\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300,\n",
    "    title='Stacked Contributions by Country'\n",
    ")\n",
    "\n",
    "chart.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make a Fancy Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the lollipop chart\n",
    "lollipop_chart = alt.Chart(map_df).transform_filter(\n",
    "    alt.datum.count > 0\n",
    ").mark_circle(size=100).encode(\n",
    "    x=alt.X('country', axis=alt.Axis(labelAngle=45)),  # Rotate x-axis labels for better readability\n",
    "    y='count',\n",
    "    color=alt.Color(\"count:Q\", \n",
    "        scale=alt.Scale(domain=[0, 2000]), \n",
    "        legend=alt.Legend(\n",
    "            title=\"No. of released Membrane Protein Structure\", \n",
    "            orient=\"bottom\", titleLimit=2000\n",
    "        )\n",
    "    ),\n",
    "    tooltip=alt.Tooltip('count:Q', title=\"No. of released Membrane Protein Structure\")\n",
    ").properties(\n",
    "    width=\"container\",\n",
    "    title='Lollipop Chart'\n",
    ")\n",
    "\n",
    "# Add the vertical lines (lollipops)\n",
    "lollipop_lines = alt.Chart(map_df).transform_filter(\n",
    "    alt.datum.count > 0\n",
    ").mark_rule(color='black').encode(\n",
    "    x='country',\n",
    "    y='count',\n",
    "    color=alt.Color(\"count:Q\", \n",
    "        scale=alt.Scale(domain=[0, 2000]), \n",
    "        legend=alt.Legend(\n",
    "            title=\"No. of released Membrane Protein Structure\", \n",
    "            orient=\"bottom\", titleLimit=2000\n",
    "        )\n",
    "    ),\n",
    "    tooltip=alt.Tooltip('count:Q', title=\"No. of released Membrane Protein Structure\")\n",
    ")\n",
    "\n",
    "# Combine the chart and lines\n",
    "chart = (lollipop_chart + lollipop_lines)\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Path to save all images for summary statistics\n",
    "directory = './summary-statistics'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    print(f\"Directory '{directory}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Directory '{directory}' already exists.\")\n",
    "\n",
    "\n",
    "request_for_group = '''\n",
    "    group,\n",
    "    group*monotopic_membrane_proteins,\n",
    "    group*transmembrane_proteins:alpha-helical,\n",
    "    group*transmembrane_proteins:beta-barrel,\n",
    "    rcsentinfo_selected_polymer_entity_types,\n",
    "    expressed_in_species*e._coli,\n",
    "    expressed_in_species*hek293_cells,\n",
    "    expressed_in_species*s._frugiperda,species,\n",
    "    expressed_in_species,processed_resolution,\n",
    "    rcsentinfo_software_programs_combined,\n",
    "    symspagroup_name_hm,rcsentinfo_molecular_weight,\n",
    "    rcsentinfo_experimental_method,\n",
    "    rcsentinfo_experimental_method*x-ray,\n",
    "    rcsentinfo_experimental_method*nmr,\n",
    "    rcsentinfo_experimental_method*em,\n",
    "    rcsentinfo_experimental_method*multiple_methods,\n",
    "    rcsentinfo_selected_polymer_entity_types*protein_(only)\n",
    "'''.replace(\"\\n\", \"\").replace(\" \", \"\")\n",
    "default_display = [\n",
    "    'taxonomic_domain',\n",
    "    'taxonomic_domain*bacteria', \n",
    "    'taxonomic_domain*eukaryota', \n",
    "    'taxonomic_domain*viruses', \n",
    "    'taxonomic_domain*unclassified', \n",
    "    'citation_country'\n",
    "]\n",
    "# Split the string on commas to create a list\n",
    "request_for_group_list = request_for_group.split(',')\n",
    "# merge array/list\n",
    "group_list = default_display + request_for_group_list\n",
    "\n",
    "#chart config\n",
    "conf = {}\n",
    "\n",
    "# Convert the list to a set to get unique elements\n",
    "unique_group_list = list(set(group_list))\n",
    "for (key, graph) in enumerate(unique_group_list):\n",
    "    group_graph, _ = pages.view_dashboard(graph, conf)\n",
    "    group_graph_chart = alt.Chart.from_dict(group_graph).properties(\n",
    "        width=1000,  # Set desired width\n",
    "        height=800  # Set desired height\n",
    "    )\n",
    "    group_graph_chart.save('./summary-statistics/' + graph + 'chart.png', scale_factor=2.0)\n",
    "print(\"images has been saved to '/summary-statistics'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate discrepencies Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DataDiscrepancyAnalyzer:\n",
    "    def __init__(self, data, reference_column, compare_columns, exceptions=None):\n",
    "        self.df = pd.DataFrame(data)\n",
    "        self.reference_column = reference_column\n",
    "        self.compare_columns = compare_columns\n",
    "        self.exceptions = exceptions if exceptions else {}\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        # Update \"Group (OPM)\" column to replace \"Bitopic proteins\" with \"Bitopic\"\n",
    "        self.df[\"Group (OPM)\"] = self.df[\"Group (OPM)\"].replace(\"Bitopic proteins\", \"Bitopic\")\n",
    "\n",
    "    def calculate_discrepancies(self):\n",
    "        discrepancies = {}\n",
    "        total_rows = len(self.df)\n",
    "\n",
    "        for col in self.compare_columns:\n",
    "            discrepancy_count = 0\n",
    "\n",
    "            for _, row in self.df.iterrows():\n",
    "                if row[self.reference_column] != row[col]:\n",
    "                    # Check if the mismatch is an exception\n",
    "                    if not (\n",
    "                        row[self.reference_column] in self.exceptions\n",
    "                        and self.exceptions[row[self.reference_column]] == row[col]\n",
    "                    ):\n",
    "                        discrepancy_count += 1\n",
    "\n",
    "            discrepancies[col] = {\n",
    "                \"discrepancies\": discrepancy_count,\n",
    "                \"percentage\": (discrepancy_count / total_rows) * 100,\n",
    "            }\n",
    "\n",
    "        return discrepancies\n",
    "\n",
    "    def interpret_discrepancies(self, discrepancies):\n",
    "        interpretation = []\n",
    "        for col, stats in discrepancies.items():\n",
    "            interpretation.append(\n",
    "                f\"{col} has {stats['discrepancies']} discrepancies, which is {stats['percentage']:.2f}% of the total entries.\"\n",
    "            )\n",
    "        return \"\\n\".join(interpretation)\n",
    "\n",
    "    def check_correct_info_availability(self):\n",
    "        def availability(row):\n",
    "            return {\n",
    "                col: row[self.reference_column] == row[col] for col in self.compare_columns\n",
    "            }\n",
    "\n",
    "        self.df[\"Correct Information Availability\"] = self.df.apply(availability, axis=1)\n",
    "\n",
    "    def run_analysis(self):\n",
    "        self.preprocess_data()\n",
    "        discrepancies = self.calculate_discrepancies()\n",
    "        interpretation = self.interpret_discrepancies(discrepancies)\n",
    "        self.check_correct_info_availability()\n",
    "        return interpretation, self.df\n",
    "\n",
    "    \n",
    "\n",
    "data = pd.read_csv(\"our_new_selection_inconsistencies.csv\")\n",
    "# Define parameters\n",
    "reference_column = \"Group (Expert)\"\n",
    "compare_columns = [\"Group (OPM)\", \"Group (MPstruc)\", \"Group (Predicted)\"]\n",
    "exceptions = {\n",
    "    \"Bitopic\": \"Transmembrane proteins:alpha-helical\",\n",
    "}\n",
    "\n",
    "# Run analysis\n",
    "analyzer = DataDiscrepancyAnalyzer(data, reference_column, compare_columns, exceptions)\n",
    "interpretation, updated_df = analyzer.run_analysis()\n",
    "\n",
    "# Print results\n",
    "print(\"Discrepancy Analysis:\\n\" + interpretation)\n",
    "print(\"\\nUpdated DataFrame with Availability Information:\")\n",
    "print(updated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Good morning!\n",
    "Discrepancy analysis (in %) with the expert results used as the reference column (Total discrepancies: 121):\n",
    "\n",
    "Group (OPM) has 23 discrepancies, which is 19.01% of the total entries.\n",
    "Group (MPstruc) has 36 discrepancies, which is 29.75% of the total entries.\n",
    "Group (Predicted) has 28 discrepancies, which is 23.14% of the total entries.\n",
    "\n",
    "Note: Based on expert input, we treated \"Bitopic\" as equivalent to \"Transmembrane proteins: alpha-helical.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"./datasets/Mpstruct_dataset.csv\")\n",
    "\n",
    "# Find duplicate entries based on 'pdb_code'\n",
    "duplicates = df[df.duplicated(subset='Pdb Code', keep=False)]\n",
    "\n",
    "# Display duplicates\n",
    "print(duplicates)\n",
    "\n",
    "# Optionally save duplicates to a file\n",
    "duplicates.to_csv('duplicates.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"./datasets/PDB_data.csv\")\n",
    "\n",
    "# Find duplicate entries based on 'pdb_code'\n",
    "duplicates = df[df.duplicated(subset='Pdb Code', keep=False)]\n",
    "\n",
    "# Display duplicates\n",
    "print(duplicates)\n",
    "\n",
    "# Optionally save duplicates to a file\n",
    "duplicates.to_csv('duplicates.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from Bio import SeqIO\n",
    "from io import StringIO\n",
    "\n",
    "def get_protein_segment(accession: str, start: int, end: int) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the full sequence for a UniProt accession and returns\n",
    "    the subsequence from residue `start` to `end` (1-based, inclusive).\n",
    "    \"\"\"\n",
    "    # 1) Download the FASTA\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{accession}.fasta\"\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    # 2) Parse with Biopython\n",
    "    seq_record = SeqIO.read(StringIO(resp.text), \"fasta\")\n",
    "\n",
    "    # 3) Slice (note Python is 0-based, UniProt is 1-based)\n",
    "    segment = seq_record.seq[start-1:end]\n",
    "    return str(segment)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: get residues 50–100 of human AKT1 (UniProt P31749)\n",
    "    acc = \"P31749\"\n",
    "    seg = get_protein_segment(acc, 50, 100)\n",
    "    print(f\"> {acc}[50–100]\\n{seg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def count_transmembrane_segments(accession: str) -> int:\n",
    "    \"\"\"\n",
    "    Fetches the UniProt entry for `accession` in JSON format,\n",
    "    then counts all features of type 'TRANSMEM'.\n",
    "    \"\"\"\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{accession}.json\"\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    print(data)\n",
    "    # UniProt JSON lists features under data['features']\n",
    "    transmem_features = [\n",
    "        f for f in data.get('features', [])\n",
    "        if f.get('type') == 'TRANSMEM'\n",
    "    ]\n",
    "    return len(transmem_features)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    acc = \"MRP1_HUMAN\"  # example: human AKT1\n",
    "    n = count_transmembrane_segments(acc)\n",
    "    print(f\"{acc} has {n} transmembrane segment(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biolib import load, login\n",
    "\n",
    "# 1) authenticate\n",
    "token=\"MpE1hFXuc1J8J4Mi3IuBrtEuRrABZEBMJNDQCEtZc98\"\n",
    "login()\n",
    "\n",
    "seq = \"MYGKIIFVLLLSEIVSISASSTTGVAMHTSTSSSVTKSYISSQTNDTHKRDTYAATPRAHEVSEISVRTVYPPEEETGERVQLAHHFSEPEITLIIFGVMAGVIGTILLISYGIRRLIKKSPSDVKPLPSPDTDVPLSSVEIENPETSDQ\"\n",
    "with open(\"myseq.fasta\", \"w\") as f:\n",
    "    f.write(\">query1\\n\" + seq + \"\\n\")\n",
    "\n",
    "deeptmhmm_app = load(\"DTU/DeepTMHMM:1.0.24\")\n",
    "job = deeptmhmm_app.run(fasta=\"myseq.fasta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_tmr_count_from_gff_text(gff_text: str) -> int:\n",
    "    \"\"\"\n",
    "    Given the contents of a GFF3 result as a single string, this function\n",
    "    returns the number of predicted transmembrane segments.\n",
    "    \n",
    "    It first tries to parse the comment line:\n",
    "        # query1 Number of predicted TMRs: X\n",
    "    If that isn’t found, it falls back to counting 'TMhelix' feature lines.\n",
    "    \"\"\"\n",
    "    # 1) Try the comment line\n",
    "    m = re.search(r\"^#.*Number of predicted TMRs:\\s*(\\d+)\", gff_text, re.MULTILINE)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    \n",
    "    # 2) Fallback: count lines where the third column is \"TMhelix\"\n",
    "    #    GFF3 is tab-delimited, so we look for lines matching: <seq>\\tTMhelix\\t\n",
    "    count = len(re.findall(r\"^[^\\t]+\\tTMhelix\\t\", gff_text, re.MULTILINE))\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "seq = \"\"\"\n",
    "MGGSSRARWVALGLGALGLLFAALGVVMILMVPSLIKQQVLKNVRIDPSSLSFGMWKEIPVPFYLSVYFFEVVNPNEVLNGQKPVVRERGPYVYREFRQKVNITFNDNDTVSFVENRSLHFQPDKSHGSESDYIVLPNILVLGGSILMESKPVSLKLMMTLALVTMGQRAFMNRTVGEILWGYDDPFVHFLNTYLPDMLPIKGKFGLFVGMNNSNSGVFTVFTGVQNFSRIHLVDKWNGLSKIDYWHSEQCNMINGTSGQMWAPFMTPESSLEFFSPEACRSMKLTYNESRVFEGIPTYRFTAPDTLFANGSVYPPNEGFCPCRESGIQNVSTCRFGAPLFLSHPHFYNADPVLSEAVLGLNPNPKEHSLFLDIHPVTGIPMNCSVKMQLSLYIKSVKGIGQTGKIEPVVLPLLWFEQSGAMGGKPLSTFYTQLVLMPQVLHYAQYVLLGLGGLLLLVPIICQLRSQEKCFLFWSGSKKGSQDKEAIQAYSESLMSPAAKGTVLQEAKL\n",
    "\"\"\"\n",
    "\n",
    "with open(\"myseq.fasta\", \"w\") as f:\n",
    "    f.write(\">query1\\n\" + seq + \"\\n\")\n",
    "\n",
    "deeptmhmm_app = load(\"DTU/DeepTMHMM:1.0.24\")\n",
    "job = deeptmhmm_app.run(fasta=\"myseq.fasta\")\n",
    "\n",
    "# job.list_output_files()\n",
    "file_obj = job.get_output_file('/deeptmhmm_results.md').get_file_handle()\n",
    "import io\n",
    "\n",
    "file_obj.seek(0)\n",
    "content = file_obj.read().decode()\n",
    "print(\"Predicted TMR count:\", extract_tmr_count_from_gff_text(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: /Users/AwotoroE-Dev/Desktop/MetaMP-Server/.mpvis/bin/python -m tmbed predict -f myseq.fasta -p myseq.pred --out-format 0 --use-gpu\n",
      "\n",
      ">query1: 2 TM segment(s)\n",
      "\n",
      "Total TM segments across all sequences: 2\n",
      "['>query1', 'MGGSSRARWVALGLGALGLLFAALGVVMILMVPSLIKQQVLKNVRIDPSSLSFGMWKEIPVPFYLSVYFFEVVNPNEVLNGQKPVVRERGPYVYREFRQKVNITFNDNDTVSFVENRSLHFQPDKSHGSESDYIVLPNILVLGGSILMESKPVSLKLMMTLALVTMGQRAFMNRTVGEILWGYDDPFVHFLNTYLPDMLPIKGKFGLFVGMNNSNSGVFTVFTGVQNFSRIHLVDKWNGLSKIDYWHSEQCNMINGTSGQMWAPFMTPESSLEFFSPEACRSMKLTYNESRVFEGIPTYRFTAPDTLFANGSVYPPNEGFCPCRESGIQNVSTCRFGAPLFLSHPHFYNADPVLSEAVLGLNPNPKEHSLFLDIHPVTGIPMNCSVKMQLSLYIKSVKGIGQTGKIEPVVLPLLWFEQSGAMGGKPLSTFYTQLVLMPQVLHYAQYVLLGLGGLLLLVPIICQLRSQEKCFLFWSGSKKGSQDKEAIQAYSESLMSPAAKGTVLQEAKL', 'SSSSS..HHHHHHHHHHHHHHHHHHHHHHHHH......................................................................................................................................................................................................................................................................................................................................................................................................................hhhhhhhhhhhhhhhhhhhhhhhhh..............................................']\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def run_predict(fasta: str,\n",
    "                out_pred: str,\n",
    "                format_code: int = 0,\n",
    "                use_gpu: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Runs `python -m tmbed predict`:\n",
    "      -f <fasta>\n",
    "      -p <out_pred>\n",
    "      --out-format <format_code>\n",
    "    Returns True on success.\n",
    "    \"\"\"\n",
    "    cmd = [\n",
    "        sys.executable, \"-m\", \"tmbed\", \"predict\",\n",
    "        \"-f\", fasta,\n",
    "        \"-p\", out_pred,\n",
    "        \"--out-format\", str(format_code)\n",
    "    ]\n",
    "    if use_gpu:\n",
    "        cmd.append(\"--use-gpu\")\n",
    "\n",
    "    print(\"Running:\", *cmd)\n",
    "    res = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    print(res.stdout)\n",
    "    print(res.stdout, end=\"\")\n",
    "    if res.returncode != 0:\n",
    "        print(\"ERROR:\", res.stderr, file=sys.stderr)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def count_tm_segments(pred_file: str) -> int:\n",
    "    \"\"\"\n",
    "    Reads a 3-line TMbed prediction file and counts\n",
    "    contiguous runs of H/h (helices) or B/b (strands).\n",
    "    \"\"\"\n",
    "    with open(pred_file) as fh:\n",
    "        lines = [l.rstrip() for l in fh if l.strip()]\n",
    "\n",
    "    # Each record is 3 lines: header, sequence, labels\n",
    "    if len(lines) % 3 != 0:\n",
    "        raise ValueError(\"Prediction file malformed (not a 3-line format).\")\n",
    "\n",
    "    total = 0\n",
    "    for i in range(0, len(lines), 3):\n",
    "        header, seq, labels = lines[i:i+3]\n",
    "        # find all contiguous runs of H/h or B/b\n",
    "        runs = re.finditer(r\"[HhBb]+\", labels)\n",
    "        count = sum(1 for _ in runs)\n",
    "        print(f\"{header}: {count} TM segment(s)\")\n",
    "        total += count\n",
    "\n",
    "    return total\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fasta   = \"myseq.fasta\"\n",
    "    out_pred = \"myseq.pred\"      # this will be in 3-line format\n",
    "    seq = \"\"\"\n",
    "        MGGSSRARWVALGLGALGLLFAALGVVMILMVPSLIKQQVLKNVRIDPSSLSFGMWKEIPVPFYLSVYFFEVVNPNEVLNGQKPVVRERGPYVYREFRQKVNITFNDNDTVSFVENRSLHFQPDKSHGSESDYIVLPNILVLGGSILMESKPVSLKLMMTLALVTMGQRAFMNRTVGEILWGYDDPFVHFLNTYLPDMLPIKGKFGLFVGMNNSNSGVFTVFTGVQNFSRIHLVDKWNGLSKIDYWHSEQCNMINGTSGQMWAPFMTPESSLEFFSPEACRSMKLTYNESRVFEGIPTYRFTAPDTLFANGSVYPPNEGFCPCRESGIQNVSTCRFGAPLFLSHPHFYNADPVLSEAVLGLNPNPKEHSLFLDIHPVTGIPMNCSVKMQLSLYIKSVKGIGQTGKIEPVVLPLLWFEQSGAMGGKPLSTFYTQLVLMPQVLHYAQYVLLGLGGLLLLVPIICQLRSQEKCFLFWSGSKKGSQDKEAIQAYSESLMSPAAKGTVLQEAKL\n",
    "    \"\"\"\n",
    "    with open(\"myseq.fasta\", \"w\") as f:\n",
    "        f.write(\">query1\\n\" + seq + \"\\n\")\n",
    "    # 1) Run prediction in format 0 (3-line, directed)\n",
    "    if run_predict(fasta, out_pred, format_code=0, use_gpu=True):\n",
    "        # 2) Count and display TM segments\n",
    "        total = count_tm_segments(out_pred)\n",
    "        print(f\"\\nTotal TM segments across all sequences: {total}\")\n",
    "        with open(out_pred) as fh:\n",
    "            lines = [l.rstrip() for l in fh if l.strip()]\n",
    "            print(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'myseq.pred'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage:\n",
    "gff_snippet = \"\"\"# query1 Length: 1255\n",
    "# query1 Number of predicted TMRs: 1\n",
    "query1\tsignal\t1\t21\t\t\t\t\n",
    "query1\toutside\t22\t653\t\t\t\t\n",
    "query1\tTMhelix\t654\t674\t\t\t\t\n",
    "query1\tinside\t675\t1255\t\t\t\t\n",
    "\"\"\"\n",
    "\n",
    "print(\"Predicted TMR count:\", extract_tmr_count_from_gff_text(content))\n",
    "# → Predicted TMR count: 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'db_params' and 'table'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Example DataFrame\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdb_code\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdb1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdb2\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m'\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m         ]\n\u001b[1;32m     12\u001b[0m     })\n\u001b[0;32m---> 14\u001b[0m     analyzer \u001b[38;5;241m=\u001b[39m \u001b[43mMultiModelAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     analyzer\u001b[38;5;241m.\u001b[39mregister(TMbedPredictor(format_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, use_gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m     16\u001b[0m     analyzer\u001b[38;5;241m.\u001b[39mregister(DeepTMHMMPredictor())  \u001b[38;5;66;03m# requires biolib\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'db_params' and 'table'"
     ]
    }
   ],
   "source": [
    "from src.AI_Packages.TMProteinPredictor import DeepTMHMMPredictor, MultiModelAnalyzer, TMbedPredictor\n",
    "import pandas as pd\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'pdb_code':['pdb1','pdb2'],\n",
    "        'sequence': [\n",
    "            \"MRPSGTAGAALLALLAALCPASRALEEKKVCQGTSNKLTQLGTFEDHFLSLQRMFNNCEVVLGNLEITYVQRNYDLSFLKTIQEVAGYVLIALNTVERIPLENLQIIRGNMYYENSYALAVLSNYDANKTGLKELPMRNLQEILHGAVRFSNNPALCNVESIQWRDIVSSDFLSNMSMDFQNHLGSCQKCDPSCPNGSCWGAGEENCQKLTKIICAQQCSGRCRGKSPSDCCHNQCAAGCTGPRESDCLVCRKFRDEATCKDTCPPLMLYNPTTYQMDVNPEGKYSFGATCVKKCPRNYVVTDHGSCVRACGADSYEMEEDGVRKCKKCEGPCRKVCNGIGIGEFKDSLSINATNIKHFKNCTSISGDLHILPVAFRGDSFTHTPPLDPQELDILKTVKEITGFLLIQAWPENRTDLHAFENLEIIRGRTKQHGQFSLAVVSLNITSLGLRSLKEISDGDVIISGNKNLCYANTINWKKLFGTSGQKTKIISNRGENSCKATGQVCHALCSPEGCWGPEPRDCVSCRNVSRGRECVDKCNLLEGEPREFVENSECIQCHPECLPQAMNITCTGRGPDNCIQCAHYIDGPHCVKTCPAGVMGENNTLVWKYADAGHVCHLCHPNCTYGCTGPGLEGCPTNGPKIPSIATGMVGALLLLLVVALGIGLFMRRRHIVRKRTLRRLLQERELVEPLTPSGEAPNQALLRILKETEFKKIKVLGSGAFGTVYKGLWIPEGEKVKIPVAIKELREATSPKANKEILDEAYVMASVDNPHVCRLLGICLTSTVQLITQLMPFGCLLDYVREHKDNIGSQYLLNWCVQIAKGMNYLEDRRLVHRDLAARNVLVKTPQHVKITDFGLAKLLGAEEKEYHAEGGKVPIKWMALESILHRIYTHQSDVWSYGVTVWELMTFGSKPYDGIPASEISSILEKGERLPQPPICTIDVYMIMVKCWMIDADSRPKFRELIIEFSKMARDPQRYLVIQGDERMHLPSPTDSNFYRALMDEEDMDDVVDADEYLIPQQGFFSSPSTSRTPLLSSLSATSNNSTVACIDRNGLQSCPIKEDSFLQRYSSDPTGALTEDSIDDTFLPVPEYINQSVPKRPAGSVQNPVYHNQPLNPAPSRDPHYQDPHSTAVGNPEYLNTVQPTCVNSTFDSPAHWAQKGSHQISLDNPDYQQDFFPKEAKPNGIFKGSTAENAEYLRVAPQSSEFIGA\",\n",
    "            \"MGGSSRARWVALGLGALGLLFAALGVVMILMVPSLIKQQVLKNVRIDPSSLSFGMWKEIPVPFYLSVYFFEVVNPNEVLNGQKPVVRERGPYVYREFRQKVNITFNDNDTVSFVENRSLHFQPDKSHGSESDYIVLPNILVLGGSILMESKPVSLKLMMTLALVTMGQRAFMNRTVGEILWGYDDPFVHFLNTYLPDMLPIKGKFGLFVGMNNSNSGVFTVFTGVQNFSRIHLVDKWNGLSKIDYWHSEQCNMINGTSGQMWAPFMTPESSLEFFSPEACRSMKLTYNESRVFEGIPTYRFTAPDTLFANGSVYPPNEGFCPCRESGIQNVSTCRFGAPLFLSHPHFYNADPVLSEAVLGLNPNPKEHSLFLDIHPVTGIPMNCSVKMQLSLYIKSVKGIGQTGKIEPVVLPLLWFEQSGAMGGKPLSTFYTQLVLMPQVLHYAQYVLLGLGGLLLLVPIICQLRSQEKCFLFWSGSKKGSQDKEAIQAYSESLMSPAAKGTVLQEAKL\"\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    analyzer = MultiModelAnalyzer()\n",
    "    analyzer.register(TMbedPredictor(format_code=0, use_gpu=False))\n",
    "    analyzer.register(DeepTMHMMPredictor())  # requires biolib\n",
    "    result_df = analyzer.analyze(df, id_col='pdb_code', seq_col='sequence', csv_out='tm_summary.csv')\n",
    "    print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AwotoroE-Dev/Desktop/MetaMP-Server/.mpvis/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from database.db import db\n",
    "from app import app\n",
    "import pandas as pd\n",
    "from src.Dashboard.services import get_tables_as_dataframe, get_table_as_dataframe\n",
    "from src.AI_Packages.TMProteinPredictor import DeepTMHMMPredictor, MultiModelAnalyzer, TMbedPredictor\n",
    "\n",
    "with app.app_context():\n",
    "    # 1) Load and merge your tables\n",
    "    table_names = ['membrane_proteins', 'membrane_protein_opm']\n",
    "    result_df = get_tables_as_dataframe(table_names, \"pdb_code\")\n",
    "    result_df_uniprot = get_table_as_dataframe(\"membrane_protein_uniprot\")\n",
    "\n",
    "    common = (set(result_df.columns) - {\"pdb_code\"}) & set(result_df_uniprot.columns)\n",
    "    right_pruned = result_df_uniprot.drop(columns=list(common))\n",
    "    all_data = pd.merge(\n",
    "        right=result_df,\n",
    "        left=right_pruned,\n",
    "        on=\"pdb_code\",\n",
    "        how=\"outer\"\n",
    "    )\n",
    "    # all_data = all_data[all_data[\"sequence_sequence\"].notna()]\n",
    "    # all_data = all_data[all_data[\"sequence_sequence\"] != \"\"]\n",
    "\n",
    "    # 2) Define your DB connection parameters\n",
    "    db_params = {\n",
    "        'host': 'localhost',\n",
    "        'port': 5432,\n",
    "        'dbname': 'mpvis_db',\n",
    "        'user': 'mpvis_user',\n",
    "        'password': 'mpvis_user'\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_cols = [\"pdb_code\", \"sequence_sequence\", \"TMbed_tm_count\", \"DeepTMHMM_tm_count\"]\n",
    "available = [c for c in required_cols if c in all_data.columns]\n",
    "all_data = all_data[required_cols]\n",
    "all_data = all_data.loc[\n",
    "    all_data[[\"TMbed_tm_count\", \"DeepTMHMM_tm_count\"]]\n",
    "        .fillna(\"\")\n",
    "        .eq(\"\")\n",
    "        .all(axis=1)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdb_code</th>\n",
       "      <th>sequence_sequence</th>\n",
       "      <th>TMbed_tm_count</th>\n",
       "      <th>DeepTMHMM_tm_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3280</th>\n",
       "      <td>5W7L</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3281</th>\n",
       "      <td>5G1J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3282</th>\n",
       "      <td>3WXV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3284</th>\n",
       "      <td>3J8E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3285</th>\n",
       "      <td>3HGC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3286</th>\n",
       "      <td>4UPC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3340</th>\n",
       "      <td>6FFV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3352</th>\n",
       "      <td>6AN7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3353</th>\n",
       "      <td>5TSI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3357</th>\n",
       "      <td>4J05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3361</th>\n",
       "      <td>3B8C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>4P6V</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365</th>\n",
       "      <td>1FUM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3369</th>\n",
       "      <td>3BZ1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3370</th>\n",
       "      <td>3ARC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3371</th>\n",
       "      <td>3CJU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pdb_code sequence_sequence  TMbed_tm_count  DeepTMHMM_tm_count\n",
       "3280     5W7L               NaN             NaN                 NaN\n",
       "3281     5G1J               NaN             NaN                 NaN\n",
       "3282     3WXV               NaN             NaN                 NaN\n",
       "3284     3J8E               NaN             NaN                 NaN\n",
       "3285     3HGC               NaN             NaN                 NaN\n",
       "3286     4UPC               NaN             NaN                 NaN\n",
       "3340     6FFV               NaN             NaN                 NaN\n",
       "3352     6AN7               NaN             NaN                 NaN\n",
       "3353     5TSI               NaN             NaN                 NaN\n",
       "3357     4J05               NaN             NaN                 NaN\n",
       "3361     3B8C               NaN             NaN                 NaN\n",
       "3362     4P6V               NaN             NaN                 NaN\n",
       "3365     1FUM               NaN             NaN                 NaN\n",
       "3369     3BZ1               NaN             NaN                 NaN\n",
       "3370     3ARC               NaN             NaN                 NaN\n",
       "3371     3CJU               NaN             NaN                 NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching PDB sequences: 100%|██████████| 93/93 [00:46<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm \n",
    "\n",
    "def fetch_pdb_entry_sequence(pdb_id: str, timeout: int = 30) -> str:\n",
    "    url = f\"https://www.rcsb.org/fasta/entry/{pdb_id.lower()}/download\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=timeout)\n",
    "        if resp.status_code != 200:\n",
    "            return None              # 404, 500, etc. → skip\n",
    "        fasta = resp.text\n",
    "    except requests.RequestException:\n",
    "        return None                  # connection problems → skip\n",
    "\n",
    "    # strip header(s) and join lines\n",
    "    return \"\".join(\n",
    "        ln.strip() for ln in fasta.splitlines() if not ln.startswith(\">\")\n",
    "    )\n",
    "\n",
    "def fill_missing_sequences(\n",
    "        df: pd.DataFrame,\n",
    "        pdb_col: str = \"pdb_code\",\n",
    "        seq_col: str = \"sequence_sequence\"\n",
    "    ) -> None:\n",
    "    if seq_col not in df.columns:\n",
    "        df[seq_col] = pd.NA\n",
    "\n",
    "    need_seq = df[seq_col].isna()\n",
    "    if need_seq.any():\n",
    "        for idx, pdb_id in tqdm(\n",
    "            df.loc[need_seq, pdb_col].items(),\n",
    "            total=need_seq.sum(),\n",
    "            desc=\"Fetching PDB sequences\",\n",
    "        ):\n",
    "            seq = fetch_pdb_entry_sequence(pdb_id)\n",
    "            if seq is not None:\n",
    "                df.at[idx, seq_col] = seq         # update in-place\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "all_data = fill_missing_sequences(all_data, pdb_col=\"pdb_code\", seq_col=\"sequence_sequence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data[all_data[\"sequence_sequence\"].notna()]\n",
    "all_data = all_data[all_data[\"sequence_sequence\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdb_code</th>\n",
       "      <th>sequence_sequence</th>\n",
       "      <th>TMbed_tm_count</th>\n",
       "      <th>DeepTMHMM_tm_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3281</th>\n",
       "      <td>4TW1</td>\n",
       "      <td>SLKINSEIKQVSEKNLDGDTKMYTRTATTSDSQKNITQSLQFNFLT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3282</th>\n",
       "      <td>6X1K</td>\n",
       "      <td>MQDGPGTLDVFVAAGWNTDNTIEITGGATYQLSPYIMVKAGYGWNN...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3283</th>\n",
       "      <td>5DL5</td>\n",
       "      <td>ANVRLQHHHHHHHLESEQSEAKGFVEDANGSILFRTGYLTRDKKQG...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3284</th>\n",
       "      <td>5DL6</td>\n",
       "      <td>ANVRLQHHHHHHHLEGSFIDNSSVELTTRNFYFDRDYQEQSAYPAA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3285</th>\n",
       "      <td>2MHL</td>\n",
       "      <td>MHEAGEFFMRAGSATVRPTEGAGGTLGSLGGFSVTNNTQLGLTFTY...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364</th>\n",
       "      <td>4UQ8</td>\n",
       "      <td>XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366</th>\n",
       "      <td>2PPS</td>\n",
       "      <td>XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3367</th>\n",
       "      <td>1QZV</td>\n",
       "      <td>XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3368</th>\n",
       "      <td>1FE1</td>\n",
       "      <td>XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3372</th>\n",
       "      <td>7VY2</td>\n",
       "      <td>ALLSFERKYRVPGGTLVGGNLFDFWVGPFYVGFFGVATFFFAALGI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pdb_code                                  sequence_sequence  \\\n",
       "3281     4TW1  SLKINSEIKQVSEKNLDGDTKMYTRTATTSDSQKNITQSLQFNFLT...   \n",
       "3282     6X1K  MQDGPGTLDVFVAAGWNTDNTIEITGGATYQLSPYIMVKAGYGWNN...   \n",
       "3283     5DL5  ANVRLQHHHHHHHLESEQSEAKGFVEDANGSILFRTGYLTRDKKQG...   \n",
       "3284     5DL6  ANVRLQHHHHHHHLEGSFIDNSSVELTTRNFYFDRDYQEQSAYPAA...   \n",
       "3285     2MHL  MHEAGEFFMRAGSATVRPTEGAGGTLGSLGGFSVTNNTQLGLTFTY...   \n",
       "...       ...                                                ...   \n",
       "3364     4UQ8  XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...   \n",
       "3366     2PPS  XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...   \n",
       "3367     1QZV  XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...   \n",
       "3368     1FE1  XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...   \n",
       "3372     7VY2  ALLSFERKYRVPGGTLVGGNLFDFWVGPFYVGFFGVATFFFAALGI...   \n",
       "\n",
       "      TMbed_tm_count  DeepTMHMM_tm_count  \n",
       "3281             NaN                 NaN  \n",
       "3282             NaN                 NaN  \n",
       "3283             NaN                 NaN  \n",
       "3284             NaN                 NaN  \n",
       "3285             NaN                 NaN  \n",
       "...              ...                 ...  \n",
       "3364             NaN                 NaN  \n",
       "3366             NaN                 NaN  \n",
       "3367             NaN                 NaN  \n",
       "3368             NaN                 NaN  \n",
       "3372             NaN                 NaN  \n",
       "\n",
       "[77 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-14 22:27:12,927 | INFO : Loaded project DTU/DeepTMHMM:1.0.24\n",
      "Running: /Users/AwotoroE-Dev/Desktop/MetaMP-Server/.mpvis/bin/python -m tmbed predict -f /var/folders/8q/rkqxjnp16t19nftpz8rmrskr0000gp/T/tmpi5w1l7u_.fasta -p /var/folders/8q/rkqxjnp16t19nftpz8rmrskr0000gp/T/tmpi5w1l7u_.fasta.pred --out-format 0\n",
      "2025-05-14 22:27:17,288 | INFO : View the result in your browser at: https://biolib.com/results/f4e48daa-ba46-4261-a838-0aa6651395a6/\n",
      "2025-05-14 22:27:18,988 | INFO : Cloud: Initializing\n",
      "2025-05-14 22:27:18,988 | INFO : Cloud: Pulling images...\n",
      "2025-05-14 22:27:18,989 | INFO : Cloud: Computing...\n",
      "Running DeepTMHMM on 10 sequences...\n",
      "Step 1/4 | Loading transformer model...\n",
      "\n",
      "Step 2/4 | Generating embeddings for sequences...\n",
      "Generating embeddings: 100% 10/10 [00:01<00:00,  5.99seq/s]\n",
      "\n",
      "Step 3/4 | Predicting topologies for sequences in batches of 1...\n",
      "Topology prediction:  80% 8/10 [00:06<00:01,  1.78seq/s]2025-05-14 22:27:49,715 | INFO : Cloud: Computation finished\n",
      "Topology prediction: 100% 10/10 [00:06<00:00,  1.49seq/s]\n",
      "\n",
      "Step 4/4 | Generating output...\n",
      "2025-05-14 22:27:51,962 | INFO : Cloud: Result Ready\n",
      "2025-05-14 22:27:51,963 | INFO : Waiting for job f4e48daa-ba46-4261-a838-0aa6651395a6 to finish...\n",
      "2025-05-14 22:27:52,513 | INFO : Job f4e48daa-ba46-4261-a838-0aa6651395a6 has finished.\n",
      "Processed batch #1: rows 1-10\n",
      "Running: /Users/AwotoroE-Dev/Desktop/MetaMP-Server/.mpvis/bin/python -m tmbed predict -f /var/folders/8q/rkqxjnp16t19nftpz8rmrskr0000gp/T/tmp_m8csl43.fasta -p /var/folders/8q/rkqxjnp16t19nftpz8rmrskr0000gp/T/tmp_m8csl43.fasta.pred --out-format 0\n",
      "2025-05-14 22:28:05,109 | INFO : View the result in your browser at: https://biolib.com/results/0902507d-10f5-43e4-b8be-f98830290559/\n",
      "2025-05-14 22:28:06,601 | INFO : Cloud: The job has been queued. Please wait...\n",
      "2025-05-14 22:28:08,974 | INFO : Cloud: The job has been queued. Please wait...\n",
      "2025-05-14 22:28:12,336 | INFO : Cloud: The job has been queued. Please wait...\n",
      "2025-05-14 22:28:16,885 | INFO : Cloud: The job has been queued. Please wait...\n",
      "2025-05-14 22:28:22,269 | INFO : Cloud: The job has been queued. Please wait...\n",
      "2025-05-14 22:28:28,866 | INFO : Cloud: The job has been queued. Please wait...\n",
      "2025-05-14 22:28:36,346 | INFO : Cloud: The job has been queued. Please wait...\n",
      "2025-05-14 22:28:44,846 | INFO : Cloud: The job has been queued. Please wait...\n",
      "2025-05-14 22:28:54,421 | INFO : Cloud: Initializing\n",
      "2025-05-14 22:28:56,617 | INFO : Cloud: Pulling images...\n",
      "2025-05-14 22:28:56,618 | INFO : Cloud: Computing...\n",
      "Running DeepTMHMM on 10 sequences...\n",
      "Step 1/4 | Loading transformer model...\n",
      "\n",
      "Step 2/4 | Generating embeddings for sequences...\n",
      "Generating embeddings: 100% 10/10 [00:01<00:00,  5.31seq/s]\n",
      "\n",
      "Step 3/4 | Predicting topologies for sequences in batches of 1...\n",
      "Topology prediction:  80% 8/10 [00:07<00:01,  1.95seq/s]2025-05-14 22:29:29,146 | INFO : Cloud: Computation finished\n",
      "Topology prediction: 100% 10/10 [00:07<00:00,  1.36seq/s]\n",
      "\n",
      "Step 4/4 | Generating output...\n",
      "2025-05-14 22:29:31,328 | INFO : Cloud: Result Ready\n",
      "2025-05-14 22:29:31,329 | INFO : Waiting for job 0902507d-10f5-43e4-b8be-f98830290559 to finish...\n",
      "2025-05-14 22:29:31,858 | INFO : Job 0902507d-10f5-43e4-b8be-f98830290559 has finished.\n",
      "Processed batch #2: rows 11-20\n",
      "Running: /Users/AwotoroE-Dev/Desktop/MetaMP-Server/.mpvis/bin/python -m tmbed predict -f /var/folders/8q/rkqxjnp16t19nftpz8rmrskr0000gp/T/tmptiwfelge.fasta -p /var/folders/8q/rkqxjnp16t19nftpz8rmrskr0000gp/T/tmptiwfelge.fasta.pred --out-format 0\n",
      "2025-05-14 22:29:35,530 | INFO : View the result in your browser at: https://biolib.com/results/df6105e6-0f4b-4c74-b573-2646dc9cb700/\n",
      "2025-05-14 22:29:37,065 | INFO : Cloud: The job has been queued. Please wait...\n",
      "2025-05-14 22:29:39,680 | INFO : Cloud: The job has been queued. Please wait...\n",
      "2025-05-14 22:29:43,320 | INFO : Cloud: The job has been queued. Please wait...\n",
      "2025-05-14 22:29:47,825 | INFO : Cloud: The job has been queued. Please wait...\n",
      "2025-05-14 22:29:53,215 | INFO : Cloud: The job has been queued. Please wait...\n",
      "2025-05-14 22:29:59,799 | INFO : Cloud: The job has been queued. Please wait...\n",
      "2025-05-14 22:30:07,297 | INFO : Cloud: The job has been queued. Please wait...\n",
      "2025-05-14 22:30:16,129 | INFO : Cloud: The job has been queued. Please wait...\n",
      "2025-05-14 22:30:25,811 | INFO : Cloud: The job has been queued. Please wait...\n",
      "2025-05-14 22:30:36,623 | INFO : Cloud: Initializing\n",
      "2025-05-14 22:30:36,635 | INFO : Cloud: Pulling images...\n",
      "2025-05-14 22:30:36,638 | INFO : Cloud: Computing...\n",
      "Running DeepTMHMM on 10 sequences...\n",
      "Step 1/4 | Loading transformer model...\n",
      "\n",
      "Step 2/4 | Generating embeddings for sequences...\n",
      "Generating embeddings: 100% 10/10 [00:09<00:00,  1.06seq/s]\n",
      "\n",
      "Step 3/4 | Predicting topologies for sequences in batches of 1...\n",
      "Topology prediction: 100% 10/10 [00:36<00:00,  3.69s/seq]\n",
      "\n",
      "Step 4/4 | Generating output...\n",
      "2025-05-14 22:31:46,552 | INFO : Cloud: Computation finished\n",
      "2025-05-14 22:31:46,556 | INFO : Cloud: Result Ready\n",
      "2025-05-14 22:31:46,556 | INFO : Waiting for job df6105e6-0f4b-4c74-b573-2646dc9cb700 to finish...\n",
      "2025-05-14 22:31:47,064 | INFO : Job df6105e6-0f4b-4c74-b573-2646dc9cb700 has finished.\n",
      "Processed batch #3: rows 21-30\n",
      "Running: /Users/AwotoroE-Dev/Desktop/MetaMP-Server/.mpvis/bin/python -m tmbed predict -f /var/folders/8q/rkqxjnp16t19nftpz8rmrskr0000gp/T/tmpdo0ca2cn.fasta -p /var/folders/8q/rkqxjnp16t19nftpz8rmrskr0000gp/T/tmpdo0ca2cn.fasta.pred --out-format 0\n",
      "2025-05-14 22:38:54,983 | INFO : View the result in your browser at: https://biolib.com/results/98c29d1d-6324-4204-bb35-6c7c4faa8488/\n",
      "2025-05-14 22:38:56,563 | INFO : Cloud: Initializing\n",
      "2025-05-14 22:38:58,746 | INFO : Cloud: Pulling images...\n",
      "2025-05-14 22:38:58,746 | INFO : Cloud: Computing...\n",
      "Running DeepTMHMM on 10 sequences...\n",
      "Step 1/4 | Loading transformer model...\n",
      "\n",
      "Step 2/4 | Generating embeddings for sequences...\n",
      "Generating embeddings: 100% 10/10 [00:03<00:00,  3.09seq/s]\n",
      "\n",
      "Step 3/4 | Predicting topologies for sequences in batches of 1...\n",
      "Topology prediction: 100% 10/10 [00:12<00:00,  1.24s/seq]\n",
      "\n",
      "Step 4/4 | Generating output...\n",
      "2025-05-14 22:39:37,759 | INFO : Cloud: Computation finished\n",
      "2025-05-14 22:39:37,761 | INFO : Cloud: Result Ready\n",
      "2025-05-14 22:39:37,763 | INFO : Waiting for job 98c29d1d-6324-4204-bb35-6c7c4faa8488 to finish...\n",
      "2025-05-14 22:39:38,367 | INFO : Job 98c29d1d-6324-4204-bb35-6c7c4faa8488 has finished.\n",
      "Processed batch #4: rows 31-40\n",
      "Running: /Users/AwotoroE-Dev/Desktop/MetaMP-Server/.mpvis/bin/python -m tmbed predict -f /var/folders/8q/rkqxjnp16t19nftpz8rmrskr0000gp/T/tmp2f7nbhay.fasta -p /var/folders/8q/rkqxjnp16t19nftpz8rmrskr0000gp/T/tmp2f7nbhay.fasta.pred --out-format 0\n",
      "2025-05-14 22:39:44,027 | INFO : View the result in your browser at: https://biolib.com/results/d0353f5a-a395-484c-95a6-ac8748b810ec/\n",
      "2025-05-14 22:39:45,783 | INFO : Cloud: Initializing\n",
      "2025-05-14 22:39:47,945 | INFO : Cloud: Pulling images...\n",
      "2025-05-14 22:39:47,948 | INFO : Cloud: Computing...\n",
      "Running DeepTMHMM on 10 sequences...\n",
      "Step 1/4 | Loading transformer model...\n",
      "\n",
      "Step 2/4 | Generating embeddings for sequences...\n",
      "Generating embeddings: 100% 10/10 [00:02<00:00,  3.62seq/s]\n",
      "\n",
      "Step 3/4 | Predicting topologies for sequences in batches of 1...\n",
      "Topology prediction: 100% 10/10 [00:10<00:00,  1.07s/seq]\n",
      "\n",
      "Step 4/4 | Generating output...\n",
      "2025-05-14 22:40:25,215 | INFO : Cloud: Computation finished\n",
      "2025-05-14 22:40:25,216 | INFO : Cloud: Result Ready\n",
      "2025-05-14 22:40:25,217 | INFO : Waiting for job d0353f5a-a395-484c-95a6-ac8748b810ec to finish...\n",
      "2025-05-14 22:40:25,781 | INFO : Job d0353f5a-a395-484c-95a6-ac8748b810ec has finished.\n",
      "Processed batch #5: rows 41-50\n",
      "Running: /Users/AwotoroE-Dev/Desktop/MetaMP-Server/.mpvis/bin/python -m tmbed predict -f /var/folders/8q/rkqxjnp16t19nftpz8rmrskr0000gp/T/tmpi7576g27.fasta -p /var/folders/8q/rkqxjnp16t19nftpz8rmrskr0000gp/T/tmpi7576g27.fasta.pred --out-format 0\n",
      "2025-05-14 22:40:29,668 | INFO : View the result in your browser at: https://biolib.com/results/76130201-d64c-4a11-b43c-f72e1bb2ba49/\n",
      "2025-05-14 22:40:31,380 | INFO : Cloud: Initializing\n",
      "2025-05-14 22:40:33,653 | INFO : Cloud: Pulling images...\n",
      "2025-05-14 22:40:33,660 | INFO : Cloud: Computing...\n",
      "Running DeepTMHMM on 10 sequences...\n",
      "Step 1/4 | Loading transformer model...\n",
      "\n",
      "Step 2/4 | Generating embeddings for sequences...\n",
      "Generating embeddings: 100% 10/10 [00:03<00:00,  3.18seq/s]\n",
      "\n",
      "Step 3/4 | Predicting topologies for sequences in batches of 1...\n",
      "Topology prediction: 100% 10/10 [00:12<00:00,  1.22s/seq]\n",
      "\n",
      "Step 4/4 | Generating output...\n",
      "2025-05-14 22:41:13,381 | INFO : Cloud: Computation finished\n",
      "2025-05-14 22:41:13,382 | INFO : Cloud: Result Ready\n",
      "2025-05-14 22:41:13,383 | INFO : Waiting for job 76130201-d64c-4a11-b43c-f72e1bb2ba49 to finish...\n",
      "2025-05-14 22:41:14,026 | INFO : Job 76130201-d64c-4a11-b43c-f72e1bb2ba49 has finished.\n",
      "Processed batch #6: rows 51-60\n",
      "Running: /Users/AwotoroE-Dev/Desktop/MetaMP-Server/.mpvis/bin/python -m tmbed predict -f /var/folders/8q/rkqxjnp16t19nftpz8rmrskr0000gp/T/tmptyqitai2.fasta -p /var/folders/8q/rkqxjnp16t19nftpz8rmrskr0000gp/T/tmptyqitai2.fasta.pred --out-format 0\n",
      "2025-05-14 22:41:33,718 | INFO : View the result in your browser at: https://biolib.com/results/1d9d9929-51df-43cf-975e-727fce12b9e1/\n",
      "2025-05-14 22:41:35,398 | INFO : Cloud: Initializing\n",
      "2025-05-14 22:41:37,654 | INFO : Cloud: Pulling images...\n",
      "2025-05-14 22:41:37,664 | INFO : Cloud: Computing...\n",
      "Running DeepTMHMM on 10 sequences...\n",
      "Step 1/4 | Loading transformer model...\n",
      "\n",
      "Step 2/4 | Generating embeddings for sequences...\n",
      "Generating embeddings: 100% 10/10 [00:02<00:00,  3.86seq/s]\n",
      "\n",
      "Step 3/4 | Predicting topologies for sequences in batches of 1...\n",
      "Topology prediction:  80% 8/10 [00:09<00:01,  1.15seq/s]2025-05-14 22:42:13,341 | INFO : Cloud: Computation finished\n",
      "Topology prediction: 100% 10/10 [00:10<00:00,  1.03s/seq]\n",
      "\n",
      "Step 4/4 | Generating output...\n",
      "2025-05-14 22:42:15,533 | INFO : Cloud: Result Ready\n",
      "2025-05-14 22:42:15,534 | INFO : Waiting for job 1d9d9929-51df-43cf-975e-727fce12b9e1 to finish...\n",
      "2025-05-14 22:42:16,178 | INFO : Job 1d9d9929-51df-43cf-975e-727fce12b9e1 has finished.\n",
      "Processed batch #7: rows 61-70\n",
      "Running: /Users/AwotoroE-Dev/Desktop/MetaMP-Server/.mpvis/bin/python -m tmbed predict -f /var/folders/8q/rkqxjnp16t19nftpz8rmrskr0000gp/T/tmpr0chiv5p.fasta -p /var/folders/8q/rkqxjnp16t19nftpz8rmrskr0000gp/T/tmpr0chiv5p.fasta.pred --out-format 0\n",
      "2025-05-14 22:42:29,375 | INFO : View the result in your browser at: https://biolib.com/results/9f5f2a58-974f-4e83-b948-0a7fb5311214/\n",
      "2025-05-14 22:42:31,045 | INFO : Cloud: Initializing\n",
      "2025-05-14 22:42:33,899 | INFO : Cloud: Pulling images...\n",
      "2025-05-14 22:42:33,904 | INFO : Cloud: Computing...\n",
      "Running DeepTMHMM on 7 sequences...\n",
      "Step 1/4 | Loading transformer model...\n",
      "\n",
      "Step 2/4 | Generating embeddings for sequences...\n",
      "Generating embeddings: 100% 7/7 [00:09<00:00,  1.40s/seq]\n",
      "\n",
      "Step 3/4 | Predicting topologies for sequences in batches of 1...\n",
      "Topology prediction:  86% 6/7 [00:35<00:04,  4.09s/seq]2025-05-14 22:43:43,837 | INFO : Cloud: Computation finished\n",
      "Topology prediction: 100% 7/7 [00:37<00:00,  5.38s/seq]\n",
      "\n",
      "Step 4/4 | Generating output...\n",
      "2025-05-14 22:43:46,024 | INFO : Cloud: Result Ready\n",
      "2025-05-14 22:43:46,035 | INFO : Waiting for job 9f5f2a58-974f-4e83-b948-0a7fb5311214 to finish...\n",
      "2025-05-14 22:43:46,593 | INFO : Job 9f5f2a58-974f-4e83-b948-0a7fb5311214 has finished.\n",
      "Processed batch #8: rows 71-77\n",
      "Saved CSV: tm_summary.csv (77 rows)\n"
     ]
    }
   ],
   "source": [
    "# 3) Instantiate the analyzer\n",
    "#    - batch_size: number of sequences per DB update\n",
    "#    - max_workers: use >1 to parallelize predictor runs (optional)\n",
    "analyzer = MultiModelAnalyzer(\n",
    "    db_params=db_params,\n",
    "    table=\"membrane_proteins\",\n",
    "    batch_size=10,\n",
    "    max_workers=2\n",
    ")\n",
    "\n",
    "analyzer.register(TMbedPredictor(format_code=0, use_gpu=False))\n",
    "analyzer.register(DeepTMHMMPredictor())\n",
    "\n",
    "result_df = analyzer.analyze(\n",
    "    df=all_data,\n",
    "    id_col=\"pdb_code\",\n",
    "    seq_col=\"sequence_sequence\",\n",
    "    csv_out=\"tm_summary.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    TM (Expert)  TMbed_tm_count  DeepTMHMM_tm_count\n",
      "TM (Expert)            1.000000        0.192115            0.193515\n",
      "TMbed_tm_count         0.192115        1.000000            0.937937\n",
      "DeepTMHMM_tm_count     0.193515        0.937937            1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dd = pd.read_csv(\"expert_annotation_predicted.csv\")\n",
    "dd[['TM (Expert)', 'TMbed_tm_count', 'DeepTMHMM_tm_count']] = dd[['TM (Expert)', 'TMbed_tm_count', 'DeepTMHMM_tm_count']].fillna(0).astype(int)\n",
    "\n",
    "corr = dd[['TM (Expert)','TMbed_tm_count','DeepTMHMM_tm_count']].corr(method='pearson')\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMbed matches Expert in 86/121 rows (71.1%).\n",
      "DeepTMHMM matches Expert in 90/121 rows (74.4%).\n",
      "DeepTMHMM matches TMbed in 106/121 rows (87.6%).\n"
     ]
    }
   ],
   "source": [
    "# Count how many times each predictor equals the expert count\n",
    "matches_tmbed = (dd['TMbed_tm_count'] == dd['TM (Expert)']).sum()\n",
    "matches_deeptmhmm = (dd['DeepTMHMM_tm_count'] == dd['TM (Expert)']).sum()\n",
    "\n",
    "matches_deeptmhmm_tmbed = (dd['DeepTMHMM_tm_count'] == dd['TMbed_tm_count']).sum()\n",
    "total = len(dd)\n",
    "\n",
    "print(f\"TMbed matches Expert in {matches_tmbed}/{total} rows \"\n",
    "      f\"({matches_tmbed/total:.1%}).\")\n",
    "print(f\"DeepTMHMM matches Expert in {matches_deeptmhmm}/{total} rows \"\n",
    "      f\"({matches_deeptmhmm/total:.1%}).\")\n",
    "print(f\"DeepTMHMM matches TMbed in {matches_deeptmhmm_tmbed}/{total} rows \"\n",
    "      f\"({matches_deeptmhmm_tmbed/total:.1%}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_tmbed  = (dd['TMbed_tm_count']  - dd['TM (Expert)']).abs().mean()\n",
    "mae_deep   = (dd['DeepTMHMM_tm_count'] - dd['TM (Expert)']).abs().mean()\n",
    "mae_deep_tm   = (dd['DeepTMHMM_tm_count'] - dd['TMbed_tm_count']).abs().mean()\n",
    "\n",
    "std_tmbed  = (dd['TMbed_tm_count']  - dd['TM (Expert)']).abs().std()\n",
    "std_deep   = (dd['DeepTMHMM_tm_count'] - dd['TM (Expert)']).abs().std()\n",
    "std_deep_tm   = (dd['DeepTMHMM_tm_count'] - dd['TMbed_tm_count']).abs().std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.322314049586777 3.355371900826446 0.18181818181818182\n",
      "12.81549509775377 12.76640089201092 0.5627314338711377\n"
     ]
    }
   ],
   "source": [
    "print(mae_deep, mae_tmbed, mae_deep_tm)\n",
    "print(std_deep, std_tmbed, std_deep_tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = dd[['TM (Expert)','TMbed_tm_count','DeepTMHMM_tm_count']].corr(method='spearman')\n",
    "corr = dd[['TM (Expert)','TMbed_tm_count','DeepTMHMM_tm_count']].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                    TM (Expert)  TMbed_tm_count  DeepTMHMM_tm_count\n",
       " TM (Expert)            1.000000        0.267926            0.372918\n",
       " TMbed_tm_count         0.267926        1.000000            0.738908\n",
       " DeepTMHMM_tm_count     0.372918        0.738908            1.000000,\n",
       "                     TM (Expert)  TMbed_tm_count  DeepTMHMM_tm_count\n",
       " TM (Expert)            1.000000        0.192115            0.193515\n",
       " TMbed_tm_count         0.192115        1.000000            0.937937\n",
       " DeepTMHMM_tm_count     0.193515        0.937937            1.000000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-f1d7768ef77f4351bb687c42224b24f1.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-f1d7768ef77f4351bb687c42224b24f1.vega-embed details,\n",
       "  #altair-viz-f1d7768ef77f4351bb687c42224b24f1.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-f1d7768ef77f4351bb687c42224b24f1\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-f1d7768ef77f4351bb687c42224b24f1\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-f1d7768ef77f4351bb687c42224b24f1\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.16.3?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.16.3\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300, \"stroke\": null}}, \"hconcat\": [{\"layer\": [{\"mark\": {\"type\": \"rect\"}, \"encoding\": {\"color\": {\"field\": \"value\", \"legend\": {\"orient\": \"right\", \"title\": \"Correlation\"}, \"scale\": {\"domain\": [0, 1], \"range\": [\"#FFFFFF\", \"#005DB9\"]}, \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -40}, \"field\": \"x\", \"title\": \"\", \"type\": \"nominal\"}, \"y\": {\"field\": \"y\", \"title\": \"\", \"type\": \"nominal\"}}}, {\"mark\": {\"type\": \"text\", \"fontSize\": 11}, \"encoding\": {\"color\": {\"value\": \"black\"}, \"text\": {\"field\": \"value\", \"format\": \".2f\", \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -40}, \"field\": \"x\", \"title\": \"\", \"type\": \"nominal\"}, \"y\": {\"field\": \"y\", \"title\": \"\", \"type\": \"nominal\"}}}], \"data\": {\"name\": \"data-5abe5b30c544b536acfe3bbfcc1bde0f\"}, \"height\": 250, \"title\": \"Pearson r\", \"width\": 250}, {\"layer\": [{\"mark\": {\"type\": \"rect\"}, \"encoding\": {\"color\": {\"field\": \"value\", \"legend\": {\"orient\": \"right\", \"title\": \"Correlation\"}, \"scale\": {\"domain\": [0, 1], \"range\": [\"#FFFFFF\", \"#005DB9\"]}, \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -40}, \"field\": \"x\", \"title\": \"\", \"type\": \"nominal\"}, \"y\": {\"field\": \"y\", \"title\": \"\", \"type\": \"nominal\"}}}, {\"mark\": {\"type\": \"text\", \"fontSize\": 11}, \"encoding\": {\"color\": {\"value\": \"black\"}, \"text\": {\"field\": \"value\", \"format\": \".2f\", \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -40}, \"field\": \"x\", \"title\": \"\", \"type\": \"nominal\"}, \"y\": {\"field\": \"y\", \"title\": \"\", \"type\": \"nominal\"}}}], \"data\": {\"name\": \"data-72658a00938bc82c897ca2a4c3e19002\"}, \"height\": 250, \"title\": \"Spearman \\u03c1\", \"width\": 250}], \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.16.3.json\", \"datasets\": {\"data-5abe5b30c544b536acfe3bbfcc1bde0f\": [{\"y\": \"TM (Expert)\", \"x\": \"TM (Expert)\", \"value\": 1.0}, {\"y\": \"TM (TMbed)\", \"x\": \"TM (Expert)\", \"value\": 0.19211457635884116}, {\"y\": \"TM (DeepTMHMM)\", \"x\": \"TM (Expert)\", \"value\": 0.1935150133902565}, {\"y\": \"TM (Expert)\", \"x\": \"TM (TMbed)\", \"value\": 0.19211457635884116}, {\"y\": \"TM (TMbed)\", \"x\": \"TM (TMbed)\", \"value\": 1.0}, {\"y\": \"TM (DeepTMHMM)\", \"x\": \"TM (TMbed)\", \"value\": 0.9379369179892398}, {\"y\": \"TM (Expert)\", \"x\": \"TM (DeepTMHMM)\", \"value\": 0.1935150133902565}, {\"y\": \"TM (TMbed)\", \"x\": \"TM (DeepTMHMM)\", \"value\": 0.9379369179892398}, {\"y\": \"TM (DeepTMHMM)\", \"x\": \"TM (DeepTMHMM)\", \"value\": 1.0}], \"data-72658a00938bc82c897ca2a4c3e19002\": [{\"y\": \"TM (Expert)\", \"x\": \"TM (Expert)\", \"value\": 1.0}, {\"y\": \"TM (TMbed)\", \"x\": \"TM (Expert)\", \"value\": 0.2679257119628923}, {\"y\": \"TM (DeepTMHMM)\", \"x\": \"TM (Expert)\", \"value\": 0.37291823689127657}, {\"y\": \"TM (Expert)\", \"x\": \"TM (TMbed)\", \"value\": 0.2679257119628923}, {\"y\": \"TM (TMbed)\", \"x\": \"TM (TMbed)\", \"value\": 1.0}, {\"y\": \"TM (DeepTMHMM)\", \"x\": \"TM (TMbed)\", \"value\": 0.7389079664051934}, {\"y\": \"TM (Expert)\", \"x\": \"TM (DeepTMHMM)\", \"value\": 0.37291823689127657}, {\"y\": \"TM (TMbed)\", \"x\": \"TM (DeepTMHMM)\", \"value\": 0.7389079664051934}, {\"y\": \"TM (DeepTMHMM)\", \"x\": \"TM (DeepTMHMM)\", \"value\": 1.0}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PNGs written with white–#005DB9 gradient.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Helper → white-to-blue (“#005DB9”) gradient\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "nice_names = {\n",
    "    \"TM (Expert)\":        \"TM (Expert)\",\n",
    "    \"TMbed_tm_count\":     \"TM (TMbed)\",\n",
    "    \"DeepTMHMM_tm_count\": \"TM (DeepTMHMM)\",\n",
    "}\n",
    "\n",
    "corr.rename(index=nice_names, columns=nice_names, inplace=True)\n",
    "rho.rename(index=nice_names,  columns=nice_names,  inplace=True)\n",
    "\n",
    "def make_heat(df, title):\n",
    "    long = (\n",
    "        df.reset_index()\n",
    "          .melt(id_vars=\"index\", var_name=\"x\", value_name=\"value\")\n",
    "          .rename(columns={\"index\": \"y\"})\n",
    "    )\n",
    "\n",
    "    base = (\n",
    "        alt.Chart(long, width=250, height=250)\n",
    "        .encode(\n",
    "            x=alt.X(\"x:N\", title=\"\", axis=alt.Axis(labelAngle=-40)),\n",
    "            y=alt.Y(\"y:N\", title=\"\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    rect = base.mark_rect().encode(\n",
    "        color=alt.Color(\n",
    "            \"value:Q\",\n",
    "            scale=alt.Scale(\n",
    "                domain=[0, 1],\n",
    "                range=[\"#FFFFFF\", \"#005DB9\"]   # ← custom palette\n",
    "            ),\n",
    "            legend=alt.Legend(title=\"Correlation\", orient=\"right\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    text = base.mark_text(fontSize=11).encode(\n",
    "        text=alt.Text(\"value:Q\", format=\".2f\"),\n",
    "        color=alt.value(\"black\"),\n",
    "    )\n",
    "\n",
    "    return (rect + text).properties(title=title)\n",
    "\n",
    "heat_corr = make_heat(corr, \"Pearson r\")\n",
    "heat_rho  = make_heat(rho,  \"Spearman ρ\")\n",
    "\n",
    "chart = (\n",
    "    alt.hconcat(heat_corr, heat_rho)\n",
    "       .resolve_scale(color=\"independent\")\n",
    "       .configure_view(stroke=None)\n",
    ")\n",
    "\n",
    "# Show in notebook / JupyterLab\n",
    "chart.display()\n",
    "\n",
    "# Export high-resolution PNGs (500 × 500 px each)\n",
    "chart.save(\"correlation_plot.png\", scale_factor=10)\n",
    "\n",
    "print(\"✓ PNGs written with white–#005DB9 gradient.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-1c5b498292e649b881d1f5f3a0321a19.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-1c5b498292e649b881d1f5f3a0321a19.vega-embed details,\n",
       "  #altair-viz-1c5b498292e649b881d1f5f3a0321a19.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-1c5b498292e649b881d1f5f3a0321a19\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-1c5b498292e649b881d1f5f3a0321a19\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-1c5b498292e649b881d1f5f3a0321a19\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.16.3?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.16.3\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300, \"stroke\": null}}, \"hconcat\": [{\"layer\": [{\"mark\": {\"type\": \"rect\"}, \"encoding\": {\"color\": {\"field\": \"value\", \"legend\": {\"orient\": \"right\", \"title\": \"Correlation\"}, \"scale\": {\"domain\": [0, 1], \"range\": [\"#FFFFFF\", \"#005DB9\"]}, \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -40}, \"field\": \"x\", \"title\": \"\", \"type\": \"nominal\"}, \"y\": {\"field\": \"y\", \"title\": \"\", \"type\": \"nominal\"}}}, {\"mark\": {\"type\": \"text\", \"fontSize\": 11}, \"encoding\": {\"color\": {\"value\": \"black\"}, \"text\": {\"field\": \"value\", \"format\": \".2f\", \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -40}, \"field\": \"x\", \"title\": \"\", \"type\": \"nominal\"}, \"y\": {\"field\": \"y\", \"title\": \"\", \"type\": \"nominal\"}}}, {\"data\": {\"name\": \"data-801c6e50e657cb0db4c52ca1452b0957\"}, \"mark\": {\"type\": \"text\", \"align\": \"left\", \"fontSize\": 14, \"fontWeight\": \"bold\", \"x\": 5, \"y\": 15}, \"encoding\": {\"text\": {\"field\": \"lab\", \"type\": \"nominal\"}}}], \"data\": {\"name\": \"data-5abe5b30c544b536acfe3bbfcc1bde0f\"}, \"height\": 250, \"title\": \"Pearson r\", \"width\": 250}, {\"layer\": [{\"mark\": {\"type\": \"rect\"}, \"encoding\": {\"color\": {\"field\": \"value\", \"legend\": {\"orient\": \"right\", \"title\": \"Correlation\"}, \"scale\": {\"domain\": [0, 1], \"range\": [\"#FFFFFF\", \"#005DB9\"]}, \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -40}, \"field\": \"x\", \"title\": \"\", \"type\": \"nominal\"}, \"y\": {\"field\": \"y\", \"title\": \"\", \"type\": \"nominal\"}}}, {\"mark\": {\"type\": \"text\", \"fontSize\": 11}, \"encoding\": {\"color\": {\"value\": \"black\"}, \"text\": {\"field\": \"value\", \"format\": \".2f\", \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"labelAngle\": -40}, \"field\": \"x\", \"title\": \"\", \"type\": \"nominal\"}, \"y\": {\"field\": \"y\", \"title\": \"\", \"type\": \"nominal\"}}}, {\"data\": {\"name\": \"data-8c7a8dec39d24894d8057f077b401684\"}, \"mark\": {\"type\": \"text\", \"align\": \"left\", \"fontSize\": 14, \"fontWeight\": \"bold\", \"x\": 5, \"y\": 15}, \"encoding\": {\"text\": {\"field\": \"lab\", \"type\": \"nominal\"}}}], \"data\": {\"name\": \"data-72658a00938bc82c897ca2a4c3e19002\"}, \"height\": 250, \"title\": \"Spearman \\u03c1\", \"width\": 250}], \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.16.3.json\", \"datasets\": {\"data-5abe5b30c544b536acfe3bbfcc1bde0f\": [{\"y\": \"TM (Expert)\", \"x\": \"TM (Expert)\", \"value\": 1.0}, {\"y\": \"TM (TMbed)\", \"x\": \"TM (Expert)\", \"value\": 0.19211457635884116}, {\"y\": \"TM (DeepTMHMM)\", \"x\": \"TM (Expert)\", \"value\": 0.1935150133902565}, {\"y\": \"TM (Expert)\", \"x\": \"TM (TMbed)\", \"value\": 0.19211457635884116}, {\"y\": \"TM (TMbed)\", \"x\": \"TM (TMbed)\", \"value\": 1.0}, {\"y\": \"TM (DeepTMHMM)\", \"x\": \"TM (TMbed)\", \"value\": 0.9379369179892398}, {\"y\": \"TM (Expert)\", \"x\": \"TM (DeepTMHMM)\", \"value\": 0.1935150133902565}, {\"y\": \"TM (TMbed)\", \"x\": \"TM (DeepTMHMM)\", \"value\": 0.9379369179892398}, {\"y\": \"TM (DeepTMHMM)\", \"x\": \"TM (DeepTMHMM)\", \"value\": 1.0}], \"data-801c6e50e657cb0db4c52ca1452b0957\": [{\"lab\": \"(A)\"}], \"data-72658a00938bc82c897ca2a4c3e19002\": [{\"y\": \"TM (Expert)\", \"x\": \"TM (Expert)\", \"value\": 1.0}, {\"y\": \"TM (TMbed)\", \"x\": \"TM (Expert)\", \"value\": 0.2679257119628923}, {\"y\": \"TM (DeepTMHMM)\", \"x\": \"TM (Expert)\", \"value\": 0.37291823689127657}, {\"y\": \"TM (Expert)\", \"x\": \"TM (TMbed)\", \"value\": 0.2679257119628923}, {\"y\": \"TM (TMbed)\", \"x\": \"TM (TMbed)\", \"value\": 1.0}, {\"y\": \"TM (DeepTMHMM)\", \"x\": \"TM (TMbed)\", \"value\": 0.7389079664051934}, {\"y\": \"TM (Expert)\", \"x\": \"TM (DeepTMHMM)\", \"value\": 0.37291823689127657}, {\"y\": \"TM (TMbed)\", \"x\": \"TM (DeepTMHMM)\", \"value\": 0.7389079664051934}, {\"y\": \"TM (DeepTMHMM)\", \"x\": \"TM (DeepTMHMM)\", \"value\": 1.0}], \"data-8c7a8dec39d24894d8057f077b401684\": [{\"lab\": \"(B)\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ correlation_plot.png written at 10× resolution.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2. Helper → white-to-#005DB9 heat-map\n",
    "# ------------------------------------------------------------\n",
    "def heat(df: pd.DataFrame, title: str) -> alt.Chart:\n",
    "    long = (\n",
    "        df.reset_index()\n",
    "          .melt(id_vars=\"index\", var_name=\"x\", value_name=\"value\")\n",
    "          .rename(columns={\"index\": \"y\"})\n",
    "    )\n",
    "    base = alt.Chart(long, width=250, height=250).encode(\n",
    "        x=alt.X(\"x:N\", title=\"\", axis=alt.Axis(labelAngle=-40)),\n",
    "        y=alt.Y(\"y:N\", title=\"\"),\n",
    "    )\n",
    "    rect = base.mark_rect().encode(\n",
    "        color=alt.Color(\"value:Q\",\n",
    "                        scale=alt.Scale(domain=[0, 1],\n",
    "                                        range=[\"#FFFFFF\", \"#005DB9\"]),\n",
    "                        legend=alt.Legend(title=\"Correlation\",\n",
    "                                          orient=\"right\")),\n",
    "    )\n",
    "    txt = base.mark_text(fontSize=11).encode(\n",
    "        text=alt.Text(\"value:Q\", format=\".2f\"),\n",
    "        color=alt.value(\"black\"),\n",
    "    )\n",
    "    return (rect + txt).properties(title=title)\n",
    "\n",
    "heat_corr = heat(corr, \"Pearson r\")\n",
    "heat_rho  = heat(rho,  \"Spearman ρ\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  Stamp panel letters\n",
    "# ------------------------------------------------------------\n",
    "def add_panel_label(chart: alt.Chart, letter: str) -> alt.LayerChart:\n",
    "    label_chart = alt.Chart(pd.DataFrame({\"lab\": [letter]})).mark_text(\n",
    "        x=5, y=15,        # pixels from top-left corner\n",
    "        align=\"left\", fontSize=14, fontWeight=\"bold\"\n",
    "    ).encode(text=\"lab:N\")\n",
    "    return (chart + label_chart)\n",
    "\n",
    "labeled_corr = add_panel_label(heat_corr, \"(A)\")\n",
    "labeled_rho  = add_panel_label(heat_rho,  \"(B)\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4.  Concatenate, show, and export hi-res PNG\n",
    "# ------------------------------------------------------------\n",
    "combo = (\n",
    "    alt.hconcat(labeled_corr, labeled_rho)\n",
    "       .resolve_scale(color=\"independent\")\n",
    "       .configure_view(stroke=None)\n",
    ")\n",
    "\n",
    "combo.display()                               # interactive in Jupyter/Lab\n",
    "combo.save(\"correlation_plot.png\", scale_factor=10)  # ~5000 × 2500 px\n",
    "heat_corr.save(\"panel_pearson.png\",  scale_factor=10)\n",
    "heat_rho.save(\"panel_spearman.png\", scale_factor=10)\n",
    "\n",
    "print(\"✓ correlation_plot.png written at 10× resolution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Error: Unable to fetch UniProt ID for 5KN7\n",
    "Error: Unable to fetch UniProt ID for 5W7L\n",
    "Error: Unable to fetch UniProt ID for 8JBQ\n",
    "Error: Unable to fetch UniProt ID for 4TW1\n",
    "Error: Unable to fetch UniProt ID for 6X1K\n",
    "Error: Unable to fetch UniProt ID for 5DL5\n",
    "Error: Unable to fetch UniProt ID for 5DL6\n",
    "Error: Unable to fetch UniProt ID for 2MHL\n",
    "Error: Unable to fetch UniProt ID for 5NEC\n",
    "Error: Unable to fetch UniProt ID for 6WIL\n",
    "Error: Unable to fetch UniProt ID for 3Q54\n",
    "Error: Unable to fetch UniProt ID for 5EFR\n",
    "Error: Unable to fetch UniProt ID for 6ODJ\n",
    "Error: Unable to fetch UniProt ID for 5WC3\n",
    "Error: Unable to fetch UniProt ID for 4P6J\n",
    "Error: Unable to fetch UniProt ID for 6B87\n",
    "Error: Unable to fetch UniProt ID for 6MQU\n",
    "Error: Unable to fetch UniProt ID for 6TJ1\n",
    "Error: Unable to fetch UniProt ID for 5G1J\n",
    "Error: Unable to fetch UniProt ID for 6XBD\n",
    "Error: Unable to fetch UniProt ID for 3WXV\n",
    "Error: Unable to fetch UniProt ID for 3A7K\n",
    "Error: Unable to fetch UniProt ID for 2KSY\n",
    "Error: Unable to fetch UniProt ID for 6CSN\n",
    "Error: Unable to fetch UniProt ID for 6LM0\n",
    "Error: Unable to fetch UniProt ID for 7PL9\n",
    "Error: Unable to fetch UniProt ID for 7SFK\n",
    "Error: Unable to fetch UniProt ID for 7W9W\n",
    "Error: Unable to fetch UniProt ID for 8GI8\n",
    "Error: Unable to fetch UniProt ID for 7KAK\n",
    "Error: Unable to fetch UniProt ID for 8SCX\n",
    "Error: Unable to fetch UniProt ID for 7CN0\n",
    "Error: Unable to fetch UniProt ID for 3OUF\n",
    "Error: Unable to fetch UniProt ID for 5V4S\n",
    "Error: Unable to fetch UniProt ID for 3J8E\n",
    "Error: Unable to fetch UniProt ID for 7K0T\n",
    "Error: Unable to fetch UniProt ID for 5GO9\n",
    "Error: Unable to fetch UniProt ID for 6JG3\n",
    "Error: Unable to fetch UniProt ID for 6NR3\n",
    "Error: Unable to fetch UniProt ID for 6BBJ\n",
    "Error: Unable to fetch UniProt ID for 2KIX\n",
    "Error: Unable to fetch UniProt ID for 6PVR\n",
    "Error: Unable to fetch UniProt ID for 3HGC\n",
    "Error: Unable to fetch UniProt ID for 5XW6\n",
    "Error: Unable to fetch UniProt ID for 4RDQ\n",
    "Error: Unable to fetch UniProt ID for 5T5N\n",
    "Error: Unable to fetch UniProt ID for 6N23\n",
    "Error: Unable to fetch UniProt ID for 4WD7\n",
    "Error: Unable to fetch UniProt ID for 6NF4\n",
    "Error: Unable to fetch UniProt ID for 6NF6\n",
    "Error: Unable to fetch UniProt ID for 7LIC\n",
    "Error: Unable to fetch UniProt ID for 4NH2\n",
    "Error: Unable to fetch UniProt ID for 3RHW\n",
    "Error: Unable to fetch UniProt ID for 8EIZ\n",
    "Error: Unable to fetch UniProt ID for 7B9F\n",
    "Error: Unable to fetch UniProt ID for 7B6W\n",
    "Error: Unable to fetch UniProt ID for 9D3G\n",
    "Error: Unable to fetch UniProt ID for 3ORG\n",
    "Error: Unable to fetch UniProt ID for 6D0J\n",
    "Error: Unable to fetch UniProt ID for 4PX7\n",
    "Error: Unable to fetch UniProt ID for 4QO2\n",
    "Error: Unable to fetch UniProt ID for 4UPC\n",
    "Error: Unable to fetch UniProt ID for 3MKT\n",
    "Error: Unable to fetch UniProt ID for 6IDP\n",
    "Error: Unable to fetch UniProt ID for 4MT1\n",
    "Error: Unable to fetch UniProt ID for 5KHN\n",
    "Error: Unable to fetch UniProt ID for 6WU0\n",
    "Error: Unable to fetch UniProt ID for 7B8P\n",
    "Error: Unable to fetch UniProt ID for 6D79\n",
    "There is an issue with : https://data.rcsb.org/rest/v1/core/entry/7UUV\n",
    "Error: Unable to fetch UniProt ID for 5M94\n",
    "Error: Unable to fetch UniProt ID for 6GZ9\n",
    "Error: Unable to fetch UniProt ID for 7V73\n",
    "Error: Unable to fetch UniProt ID for 5A1S\n",
    "Error: Unable to fetch UniProt ID for 4N7W\n",
    "Error: Unable to fetch UniProt ID for 3RLB\n",
    "Error: Unable to fetch UniProt ID for 6FFV\n",
    "Error: Unable to fetch UniProt ID for 6AN7\n",
    "Error: Unable to fetch UniProt ID for 5TSI\n",
    "Error: Unable to fetch UniProt ID for 6D3R\n",
    "Error: Unable to fetch UniProt ID for 4OAA\n",
    "Error: Unable to fetch UniProt ID for 6EXS\n",
    "Error: Unable to fetch UniProt ID for 4J05\n",
    "Error: Unable to fetch UniProt ID for 3WDO\n",
    "Error: Unable to fetch UniProt ID for 6IU3\n",
    "Error: Unable to fetch UniProt ID for 6TDU\n",
    "Error: Unable to fetch UniProt ID for 3B8C\n",
    "Error: Unable to fetch UniProt ID for 7QBZ\n",
    "Error: Unable to fetch UniProt ID for 4P6V\n",
    "Error: Unable to fetch UniProt ID for 3M9C\n",
    "Error: Unable to fetch UniProt ID for 4UQ8\n",
    "Error: Unable to fetch UniProt ID for 1FUM\n",
    "There is an issue with : https://data.rcsb.org/rest/v1/core/entry/7ROW\n",
    "Error: Unable to fetch UniProt ID for 6QQ5\n",
    "Error: Unable to fetch UniProt ID for 8BGW\n",
    "Error: Unable to fetch UniProt ID for 6T6V\n",
    "Error: Unable to fetch UniProt ID for 2PPS\n",
    "Error: Unable to fetch UniProt ID for 1QZV\n",
    "Error: Unable to fetch UniProt ID for 1FE1\n",
    "Error: Unable to fetch UniProt ID for 3BZ1\n",
    "Error: Unable to fetch UniProt ID for 3ARC\n",
    "Error: Unable to fetch UniProt ID for 3CJU\n",
    "Error: Unable to fetch UniProt ID for 7VY2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading latest file: datasets/PDB_data_2025-05-19.csv (date: 2025-05-19)\n",
      "                                        audit_author Pdb Code   Is Replaced  \\\n",
      "0  [{'name': 'Picot, D.', 'pdbx_ordinal': 1}, {'n...     1PRH  Not Replaced   \n",
      "1  [{'name': 'Loll, P.J.', 'pdbx_ordinal': 1}, {'...     1PTH  Not Replaced   \n",
      "2  [{'name': 'Picot, D.', 'pdbx_ordinal': 1}, {'n...     1CQE  Not Replaced   \n",
      "3  [{'name': 'Loll, P.J.', 'pdbx_ordinal': 1}, {'...     1EQG  Not Replaced   \n",
      "4  [{'name': 'Loll, P.J.', 'pdbx_ordinal': 1}, {'...     1EBV  Not Replaced   \n",
      "\n",
      "  PDB Code Changed uniprot_id  \\\n",
      "0              NaN     P05979   \n",
      "1              NaN     P05979   \n",
      "2              NaN     P05979   \n",
      "3              NaN     P05979   \n",
      "4              NaN     P05979   \n",
      "\n",
      "                                            citation  \\\n",
      "0  [{'country': 'UK', 'id': 'primary', 'journal_a...   \n",
      "1  [{'country': 'US', 'id': 'primary', 'journal_a...   \n",
      "2  [{'country': 'UK', 'id': 'primary', 'journal_a...   \n",
      "3  [{'country': 'US', 'id': 'primary', 'journal_a...   \n",
      "4  [{'country': 'US', 'id': 'primary', 'journal_a...   \n",
      "\n",
      "                                           database2  \\\n",
      "0  [{'database_code': '1PRH', 'database_id': 'PDB...   \n",
      "1  [{'database_code': '1PTH', 'database_id': 'PDB...   \n",
      "2  [{'database_code': '1CQE', 'database_id': 'PDB...   \n",
      "3  [{'database_code': '1EQG', 'database_id': 'PDB...   \n",
      "4  [{'database_code': '1EBV', 'database_id': 'PDB...   \n",
      "\n",
      "                                              diffrn  \\\n",
      "0                   [{'crystal_id': '1', 'id': '1'}]   \n",
      "1                   [{'crystal_id': '1', 'id': '1'}]   \n",
      "2  [{'ambient_temp': 277.0, 'crystal_id': '1', 'i...   \n",
      "3  [{'ambient_temp': 120.0, 'crystal_id': '1', 'i...   \n",
      "4  [{'ambient_temp': 100.0, 'crystal_id': '1', 'i...   \n",
      "\n",
      "                                    diffrn_radiation  \\\n",
      "0  [{'diffrn_id': '1', 'pdbx_scattering_type': 'x...   \n",
      "1  [{'diffrn_id': '1', 'pdbx_monochromatic_or_lau...   \n",
      "2  [{'diffrn_id': '1', 'monochromator': 'GRAPHITE...   \n",
      "3  [{'diffrn_id': '1', 'pdbx_diffrn_protocol': 'S...   \n",
      "4  [{'diffrn_id': '1', 'pdbx_diffrn_protocol': 'S...   \n",
      "\n",
      "                                               exptl  ...  \\\n",
      "0                  [{'method': 'X-RAY DIFFRACTION'}]  ...   \n",
      "1                  [{'method': 'X-RAY DIFFRACTION'}]  ...   \n",
      "2  [{'crystals_number': 4, 'method': 'X-RAY DIFFR...  ...   \n",
      "3  [{'crystals_number': 1, 'method': 'X-RAY DIFFR...  ...   \n",
      "4  [{'crystals_number': 1, 'method': 'X-RAY DIFFR...  ...   \n",
      "\n",
      "  rcsb_entry_info_ndb_struct_conf_na_feature_combined  \\\n",
      "0                                                NaN    \n",
      "1                                                NaN    \n",
      "2                                                NaN    \n",
      "3                                                NaN    \n",
      "4                                                NaN    \n",
      "\n",
      "  pdbx_serial_crystallography_sample_delivery_injection  \\\n",
      "0                                                NaN      \n",
      "1                                                NaN      \n",
      "2                                                NaN      \n",
      "3                                                NaN      \n",
      "4                                                NaN      \n",
      "\n",
      "  pdbx_serial_crystallography_sample_delivery_fixed_target  \\\n",
      "0                                                NaN         \n",
      "1                                                NaN         \n",
      "2                                                NaN         \n",
      "3                                                NaN         \n",
      "4                                                NaN         \n",
      "\n",
      "  pdbx_serial_crystallography_data_reduction  \\\n",
      "0                                        NaN   \n",
      "1                                        NaN   \n",
      "2                                        NaN   \n",
      "3                                        NaN   \n",
      "4                                        NaN   \n",
      "\n",
      "  pdbx_serial_crystallography_measurement em_staining  \\\n",
      "0                                     NaN         NaN   \n",
      "1                                     NaN         NaN   \n",
      "2                                     NaN         NaN   \n",
      "3                                     NaN         NaN   \n",
      "4                                     NaN         NaN   \n",
      "\n",
      "  pdbx_database_status_methods_development_category  \\\n",
      "0                                               NaN   \n",
      "1                                               NaN   \n",
      "2                                               NaN   \n",
      "3                                               NaN   \n",
      "4                                               NaN   \n",
      "\n",
      "  pdbx_vrpt_summary_rnasuiteness em2d_crystal_entity  \\\n",
      "0                            NaN                 NaN   \n",
      "1                            NaN                 NaN   \n",
      "2                            NaN                 NaN   \n",
      "3                            NaN                 NaN   \n",
      "4                            NaN                 NaN   \n",
      "\n",
      "  pdbx_nmr_ensemble_torsion_angle_constraint_violation_method  \n",
      "0                                                NaN           \n",
      "1                                                NaN           \n",
      "2                                                NaN           \n",
      "3                                                NaN           \n",
      "4                                                NaN           \n",
      "\n",
      "[5 rows x 205 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. List all matching files\n",
    "pattern = \"datasets/PDB_data_*.csv\"\n",
    "files = glob.glob(pattern)\n",
    "\n",
    "# 2. Extract dates and find the latest\n",
    "date_file_map = {}\n",
    "for f in files:\n",
    "    m = re.search(r\"PDB_data_(\\d{4}-\\d{2}-\\d{2})\\.csv$\", f)\n",
    "    if m:\n",
    "        dt = datetime.strptime(m.group(1), \"%Y-%m-%d\").date()\n",
    "        date_file_map[dt] = f\n",
    "\n",
    "if not date_file_map:\n",
    "    raise FileNotFoundError(f\"No files matching {pattern}\")\n",
    "\n",
    "latest_date = max(date_file_map)\n",
    "latest_file = date_file_map[latest_date]\n",
    "\n",
    "print(f\"Loading latest file: {latest_file} (date: {latest_date})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mpvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
